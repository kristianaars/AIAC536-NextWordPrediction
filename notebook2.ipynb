{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Environment"
   ],
   "metadata": {
    "id": "RIG4mmKB0ZC7"
   },
   "id": "RIG4mmKB0ZC7"
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kristian.aars/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/kristian.aars/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kristian.aars/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kristian.aars/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f95d79b0e4965d34",
    "outputId": "11f72bc2-2072-48e0-c6f1-6a2fae9c064d",
    "ExecuteTime": {
     "end_time": "2023-11-28T10:48:00.567863Z",
     "start_time": "2023-11-28T10:48:00.274706Z"
    }
   },
   "id": "f95d79b0e4965d34"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install autocorrect tensorflow numpy keras regex pyyaml h5py contractions pandarallel"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sO636A_4vaWZ",
    "outputId": "c8352f0a-238d-420c-8af4-8e3f59cc90aa"
   },
   "id": "sO636A_4vaWZ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Download blogtext.csv (700 000 blog posts)\n",
    "!gdown 1PJbVYUmRr0_HTwGNtplnu8lG-UCDoXZJ\n",
    "\n",
    "# Download blogtext_cleaned.csv (10 000 blog posts, 43 500 sentences)\n",
    "!gdown 1qI-TZrQ_D0S0g7O3l_qKKA7f4l70jlmf"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7DRnSbjuAWz",
    "outputId": "7741e918-67df-4f9a-a205-3f1bfe53b3b8"
   },
   "id": "t7DRnSbjuAWz",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "id": "ox3MrBPE0eLB"
   },
   "id": "ox3MrBPE0eLB"
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "254b9bd8-ff76-4617-fcd3-a28d8334e1e3",
    "ExecuteTime": {
     "end_time": "2023-11-28T11:04:10.849122Z",
     "start_time": "2023-11-28T11:04:03.908125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.87 s, sys: 491 ms, total: 6.36 s\n",
      "Wall time: 6.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Import dataset from CSV\n",
    "\n",
    "df = pd.read_csv('blogtext.csv').head(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": "        id gender  age              topic      sign          date  \\\n0  2059027   male   15            Student       Leo   14,May,2004   \n1  2059027   male   15            Student       Leo   13,May,2004   \n2  2059027   male   15            Student       Leo   12,May,2004   \n3  2059027   male   15            Student       Leo   12,May,2004   \n4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n5  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n6  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n7  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n8  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n9  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n\n                                                text  \n0             Info has been found (+/- 100 pages,...  \n1             These are the team members:   Drewe...  \n2             In het kader van kernfusie op aarde...  \n3                   testing!!!  testing!!!            \n4               Thanks to Yahoo!'s Toolbar I can ...  \n5               I had an interesting conversation...  \n6               Somehow Coca-Cola has a way of su...  \n7               If anything, Korea is a country o...  \n8               Take a read of this news article ...  \n9               I surf the English news sites a l...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>topic</th>\n      <th>sign</th>\n      <th>date</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>14,May,2004</td>\n      <td>Info has been found (+/- 100 pages,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>13,May,2004</td>\n      <td>These are the team members:   Drewe...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>12,May,2004</td>\n      <td>In het kader van kernfusie op aarde...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>12,May,2004</td>\n      <td>testing!!!  testing!!!</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>11,June,2004</td>\n      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>I had an interesting conversation...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>Somehow Coca-Cola has a way of su...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>If anything, Korea is a country o...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>Take a read of this news article ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>09,June,2004</td>\n      <td>I surf the English news sites a l...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(10)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "377b4d2ee44e3e05",
    "outputId": "f118cebf-da94-4f0c-af2d-8bcf3cffe7e1",
    "ExecuteTime": {
     "end_time": "2023-11-28T11:04:10.858496Z",
     "start_time": "2023-11-28T11:04:10.855096Z"
    }
   },
   "id": "377b4d2ee44e3e05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenize sentences"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d565cedce4c8d393"
   },
   "id": "d565cedce4c8d393"
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.45 s, sys: 75.7 ms, total: 5.53 s\n",
      "Wall time: 5.89 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "(50000, 7)"
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.text = df.text.transform(lambda t: nltk.sent_tokenize(t))\n",
    "df.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6450521ad0458ab3",
    "outputId": "c11308b2-4248-446b-91d5-6bb28835cbd0",
    "ExecuteTime": {
     "end_time": "2023-11-28T11:04:16.758614Z",
     "start_time": "2023-11-28T11:04:10.895908Z"
    }
   },
   "id": "6450521ad0458ab3"
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(674967, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(674792, 7)"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.explode('text')\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "mask = df['text'].apply(lambda x: isinstance(x, str))\n",
    "df = df[mask]\n",
    "\n",
    "df.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35e293b7db4fb277",
    "outputId": "5489d459-c898-4994-9a3a-33172a5a57a4",
    "ExecuteTime": {
     "end_time": "2023-11-28T11:04:17.011263Z",
     "start_time": "2023-11-28T11:04:16.769409Z"
    }
   },
   "id": "35e293b7db4fb277"
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "#df.text.to_csv(\"blogtext-sentence_tokenized.csv\")"
   ],
   "metadata": {
    "id": "54052f0d1e6589d3",
    "ExecuteTime": {
     "end_time": "2023-11-28T11:04:17.011377Z",
     "start_time": "2023-11-28T11:04:17.003251Z"
    }
   },
   "id": "54052f0d1e6589d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepearing data for training"
   ],
   "metadata": {
    "collapsed": false,
    "id": "1aa0d2bb6fe1f060"
   },
   "id": "1aa0d2bb6fe1f060"
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "from keras.src.preprocessing.text import Tokenizer\n",
    "from keras.src.utils import pad_sequences, to_categorical\n",
    "import re\n",
    "import random\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import words as en_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "from autocorrect import Speller\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "id": "a8accc7b14d2daeb",
    "ExecuteTime": {
     "end_time": "2023-11-28T11:04:17.272014Z",
     "start_time": "2023-11-28T11:04:17.007206Z"
    }
   },
   "id": "a8accc7b14d2daeb"
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "def has_url(sentence):\n",
    "    doc = spacy_nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.like_url:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_email(sentence):\n",
    "    doc = spacy_nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.like_email:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_phonenumber(sentence):\n",
    "    doc = spacy_nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.like_num:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def autocorrect_corpus(corpus):\n",
    "    speller = Speller(lang='en')\n",
    "    return corpus.transform(lambda s: speller(s['text']))\n",
    "\n",
    "def has_non_lexi_word(sentence):\n",
    "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "    lemmatized_doc = spacy_nlp(sentence)\n",
    "    \n",
    "    english_words = en_words.words()\n",
    "    \n",
    "    for token in lemmatized_doc:\n",
    "        word = token.lemma_.lower()\n",
    "        \n",
    "        if word not in english_words:                               \n",
    "            #print(\"Found non-english word {0}\".format(word))\n",
    "            return True\n",
    "\n",
    "def contains_number(sentence):\n",
    "    return any(char.isdigit() for char in sentence)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    # Tokenize the input text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Get the list of English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words from the list of words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "def word_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "def clean_corpus(corpus_df, rm_sentence_phone=True, rm_sentence_email=True, rm_sentence_url=True, rm_non_lexi_word_sentence=True, rm_contains_num=True, min_word_count=2, remove_stopwords=True, prob_remove_stopword=0.5):\n",
    "    # Remove sentences with personal details as specified by function parameters\n",
    "    def remove_sentences_condition(row):\n",
    "        if word_count(row['text']) < min_word_count: return False\n",
    "        elif rm_sentence_phone and has_phonenumber(row['text']): return False\n",
    "        elif rm_contains_num and contains_number(row['text']): return False\n",
    "        elif rm_sentence_email and has_email(row['text']): return False\n",
    "        elif rm_sentence_url and has_url(row['text']): return False\n",
    "        elif rm_non_lexi_word_sentence and has_non_lexi_word(row['text']): return False\n",
    "        else: return True\n",
    "    \n",
    "    def clean_text(text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "    \n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "        # Tokenize\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # Remove stop words\n",
    "        if remove_stopwords:\n",
    "            if random.random() < prob_remove_stopword:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                words = [word for word in words if word not in stop_words]\n",
    "                \n",
    "        # Join the cleaned words back into a sentence\n",
    "        cleaned_text = ' '.join(words)\n",
    "    \n",
    "        return cleaned_text\n",
    "    \n",
    "    pre_rem_size = corpus_df.shape[0]\n",
    "    pandarallel.initialize()\n",
    "    corpus_df['text'] = corpus_df['text'].apply(clean_text)\n",
    "    corpus_df = corpus_df[corpus_df.parallel_apply(remove_sentences_condition, axis=1)]\n",
    "    sen_removed = pre_rem_size - corpus_df.shape[0]\n",
    "    print(\"Removed {0} sentences because they contained email, phone, or url(s)\".format(sen_removed))\n",
    "\n",
    "    # Run autocorrect to fix text-typos\n",
    "    # autocorrect_corpus(corpus_df)\n",
    "\n",
    "    # En løsning er å fjerne alle ord som ikke eksisterer i det engelske vokabularet.\n",
    "\n",
    "    return corpus_df\n"
   ],
   "metadata": {
    "id": "cb6ef90035ef7065",
    "ExecuteTime": {
     "end_time": "2023-11-28T11:04:17.284125Z",
     "start_time": "2023-11-28T11:04:17.277430Z"
    }
   },
   "id": "cb6ef90035ef7065"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "df = clean_corpus(df, rm_sentence_phone=False, rm_sentence_url=False, rm_sentence_email=False, remove_stopwords=True, rm_non_lexi_word_sentence=True, prob_remove_stopword=0.80)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7284a44ceb21d9ee",
    "outputId": "317a6ce0-783b-4679-a4b1-35452208afdb",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-28T11:04:17.279743Z"
    }
   },
   "id": "7284a44ceb21d9ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df\n",
    "\n",
    "# Remove sentences with less than two words\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5ed172ed9c897eb5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv(\"blogtext_cleaned.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2daf068cf13ab092"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('blogtext_cleaned.csv')\n",
    "\n",
    "#df['text'] = df['text'].apply(remove_stop_words)\n",
    "\n",
    "df = df['text'].head(100)\n",
    "\n",
    "df.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6df1f532b86fec3",
    "outputId": "b02b5230-9ecd-4dc8-d164-d1f031a2edbd",
    "is_executing": true
   },
   "id": "b6df1f532b86fec3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "770b33009bff8548",
    "outputId": "283cdf04-e1a8-41e4-9361-e912c5cbfcb9",
    "is_executing": true
   },
   "id": "770b33009bff8548"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentence_list = df.tolist()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence_list)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ],
   "metadata": {
    "id": "52eab5627e64f910",
    "is_executing": true
   },
   "id": "52eab5627e64f910"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "c_dict = {}\n",
    "\n",
    "for s in df:\n",
    "    print(s)\n",
    "    wrds = s.split()\n",
    "    for w in wrds:\n",
    "        if w in c_dict.keys():\n",
    "            c_dict[w] += 1\n",
    "        else:\n",
    "            c_dict[w] = 1\n",
    "            \n",
    "c_dict = dict(sorted(c_dict.items(), key=lambda item: item[1]))\n",
    "\n",
    "# Extract words and counts\n",
    "words = list(c_dict.keys())\n",
    "counts = list(c_dict.values())\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, counts, color='blue')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Word Count Distribution')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.show()\n",
    "\n",
    "c_dict"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "993fdf65cc43e61c"
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "data": {
      "text/plain": "700"
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11d1f4229e029de6",
    "outputId": "450e358c-d89d-4af8-ffd1-8f41021eafbb",
    "ExecuteTime": {
     "end_time": "2023-11-28T11:03:34.981028Z",
     "start_time": "2023-11-28T11:03:34.977411Z"
    }
   },
   "id": "11d1f4229e029de6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff31f871b9c4f0e9",
    "outputId": "ecbe00fe-f779-4280-ce68-50cb421a6a16"
   },
   "id": "ff31f871b9c4f0e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_gram_list = []\n",
    "n_gram_length = 3\n",
    "\n",
    "for line in sentence_list:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    for n in range(2, n_gram_length):\n",
    "        n_grams = ngrams(token_list, n)\n",
    "        n_gram_list.extend(np.asarray([*n_grams]))\n",
    "\n",
    "# Padding\n",
    "n_gram_list = np.array(pad_sequences(\n",
    "    n_gram_list,\n",
    "    maxlen=n_gram_length,\n",
    "    padding='pre'\n",
    "))"
   ],
   "metadata": {
    "id": "a636c919dc7fcba1"
   },
   "id": "a636c919dc7fcba1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = n_gram_list[:, :-1]\n",
    "y = n_gram_list[:, -1]\n",
    "\n",
    "y = to_categorical(y, num_classes=total_words)"
   ],
   "metadata": {
    "id": "b254b0558e1de286"
   },
   "id": "b254b0558e1de286"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build and Train Model"
   ],
   "metadata": {
    "id": "Zjf-vBhl0oHA"
   },
   "id": "Zjf-vBhl0oHA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.src.layers import Embedding, GRU, Dense, LSTM\n",
    "from keras import Sequential\n",
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=n_gram_length-1))\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(256))\n",
    "\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96b229d0057fd8b5",
    "outputId": "c9f23548-9050-4580-9f3b-6bef82dc4343"
   },
   "id": "96b229d0057fd8b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "checkpoint_path = \"checkpoints/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\" + \"model_checkpoint_{epoch:02d}.h5\"\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(checkpoint_path, save_freq=500, verbose=1)\n",
    "\n",
    "model.fit(X, y,\n",
    "          epochs=500, verbose=1,\n",
    "          callbacks=[tensorboard_callback, checkpoint_callback])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63a05607452391c3",
    "outputId": "ba06257a-8d6e-4401-ecdf-42babe358acb"
   },
   "id": "63a05607452391c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model.save(\"models/model_{0}.h5\".format(datetime.now()).replace(\" \", \"_\"), )\n",
    "\n",
    "with open(\"models/tokenizer_{0}.pickle\".format(datetime.now()).replace(\" \", \"_\"), 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c96235814e505f4b",
    "outputId": "a2298948-0f60-453c-aa28-5ee03647469c"
   },
   "id": "c96235814e505f4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed_text = \"Hello there, do you i am\"\n",
    "next_words = 1\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences(\n",
    "        [token_list],\n",
    "        maxlen=2,\n",
    "        padding='pre'\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(token_list)\n",
    "    pred_word = tokenizer.index_word[np.argmax(predictions)]\n",
    "    seed_text += \" \" + pred_word\n",
    "\n",
    "print(\"Next predicted words: \", seed_text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f03606a6d69bdd6c",
    "outputId": "232b2b31-9518-4928-9342-36703d255daa"
   },
   "id": "f03606a6d69bdd6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e3f770db89be1f94"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "V100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
