{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13fdc1ec4634c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install gdown pandas nltk pandarallel spacy autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c40973d657442",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15fc659ba8cfb70",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Download base dataset\n",
    "Dataset is from https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus. Contains over 700000 blog posts with metadata about author and blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download blogtext.csv (700 000 blog posts)\n",
    "!gdown 1PJbVYUmRr0_HTwGNtplnu8lG-UCDoXZJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2fd6fbd9046ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15f3e001c04e6c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Import from csv-file using Pandas\n",
    "Specify the number of blog posts to process, as the processing can take time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69fdda6e5caf3c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_BLOGPOSTS = 10000\n",
    "\n",
    "df = pd.read_csv('blogtext.csv', nrows=N_BLOGPOSTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768a4128ba118dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenize sentences\n",
    "This splits all the sentences into arrays, and then explodes the array back to the dataframe as separate sentences. After this, all sentences which are not recognized as strings are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ca54eb19f5076",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.text = df.text.transform(lambda t: nltk.sent_tokenize(t))\n",
    "\n",
    "# Explode array into separate rows\n",
    "df = df.explode('text')\n",
    "\n",
    "# Remove all sentences not recognized as strings (numbers, lists etc.)\n",
    "mask = df['text'].apply(lambda x: isinstance(x, str))\n",
    "df = df[mask]\n",
    "\n",
    "# Remove all sentences not containing anything\n",
    "df = df[df['text'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4994b2a043eb3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocess and clean text\n",
    "This step will clean the text contents and remove unwanted blog-posts from the dataset. Blogposts containing non-english words may be removed and a set percentage of stop-words can be removed to reduce their occurrence in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986a5cfe5ac73c1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Specify cleaning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff1c90c5c1af",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remove_stopwords=False\n",
    "rem_stopword_percent=0.00\n",
    "remove_sentence_with_unknown_words=True\n",
    "min_word_count=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552f1b8adec012e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import words as en_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69260ed47c5aa6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define some methods for text-cleaning and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8281b03a3fd4c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def contains_non_lexi_word(sentence):\n",
    "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "    lemmatized_doc = spacy_nlp(sentence)\n",
    "    \n",
    "    english_words = en_words.words()\n",
    "    \n",
    "    for token in lemmatized_doc:\n",
    "        word = token.lemma_.lower()\n",
    "        \n",
    "        if word not in english_words:                               \n",
    "            #print(\"Found non-english word {0}\".format(word))\n",
    "            return True\n",
    "        \n",
    "def contains_number(sentence):\n",
    "    return any(char.isdigit() for char in sentence)\n",
    "\n",
    "def word_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e691fd1384733",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_corpus(corpus_df, rm_contains_unknown_wrd=True, rm_contains_num=True, min_word_count=2, remove_stopwords=True, prob_remove_stopword=0.1):\n",
    "    \n",
    "    # Function which returns wether to keep a sentence in the dataset or not based on given parameters\n",
    "    def keep_sentence(row):\n",
    "        if word_count(row['text']) < min_word_count:\n",
    "            #print(\"{0} is too short\".format(row['text']))\n",
    "            return False\n",
    "        elif rm_contains_num and contains_number(row['text']):\n",
    "            #print(\"{0} contains number\".format(row['text']))\n",
    "            return False\n",
    "        elif rm_contains_unknown_wrd and contains_non_lexi_word(row['text']):\n",
    "            #print(\"{0} contains unknown words\".format(row['text']))\n",
    "            return False\n",
    "        else:\n",
    "            #print(\"{0} is ok\".format(row['text']))\n",
    "            return True\n",
    "    \n",
    "    # Function to normalize and clean the text\n",
    "    def clean_text(text):\n",
    "        # Convert to lowercase for better normalization\n",
    "        text = text.lower()\n",
    "    \n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "        # Tokenize the words\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # Remove stop words\n",
    "        if remove_stopwords:\n",
    "            if random.random() < prob_remove_stopword:\n",
    "                words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # Join the cleaned words back into a sentence\n",
    "        cleaned_text = ' '.join(words)\n",
    "    \n",
    "        return cleaned_text\n",
    "\n",
    "    pre_rem_size = corpus_df.shape[0]\n",
    "    pandarallel.initialize()\n",
    "    corpus_df['text'] = corpus_df['text'].apply(clean_text)\n",
    "    corpus_df = corpus_df[corpus_df.parallel_apply(keep_sentence, axis=1)]\n",
    "    sen_removed = pre_rem_size - corpus_df.shape[0]\n",
    "    print(\"Removed {0} sentences because they did not conform to the sentence specifications\".format(sen_removed))\n",
    "    \n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e51927426dc5d9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Let the cleaning begin\n",
    "This might take some while. Please be patient. 5000 blog posts takes around 10 minutes on a M1 Pro MacBook Pro 14\" laptop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27a77b0503c2aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = clean_corpus(df, remove_stopwords=remove_stopwords, rm_contains_unknown_wrd=remove_sentence_with_unknown_words, prob_remove_stopword=rem_stopword_percent, min_word_count=min_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ae26913074e40",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lastly, save the file so it can be loaded for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc788b969487ded",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"blogtext_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
