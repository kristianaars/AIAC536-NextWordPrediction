{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in ./venv/lib/python3.11/site-packages (4.7.1)\r\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (2.1.1)\r\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.11/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: pandarallel in ./venv/lib/python3.11/site-packages (1.6.5)\r\n",
      "Requirement already satisfied: spacy in ./venv/lib/python3.11/site-packages (3.7.1)\r\n",
      "Requirement already satisfied: autocorrect in ./venv/lib/python3.11/site-packages (2.6.1)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from gdown) (3.13.1)\r\n",
      "Requirement already satisfied: requests[socks] in ./venv/lib/python3.11/site-packages (from gdown) (2.31.0)\r\n",
      "Requirement already satisfied: six in ./venv/lib/python3.11/site-packages (from gdown) (1.16.0)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.11/site-packages (from gdown) (4.66.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.11/site-packages (from gdown) (4.12.2)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./venv/lib/python3.11/site-packages (from pandas) (1.26.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: click in ./venv/lib/python3.11/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.11/site-packages (from nltk) (1.3.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.11/site-packages (from nltk) (2023.10.3)\r\n",
      "Requirement already satisfied: dill>=0.3.1 in ./venv/lib/python3.11/site-packages (from pandarallel) (0.3.7)\r\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.11/site-packages (from pandarallel) (5.9.5)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/lib/python3.11/site-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/lib/python3.11/site-packages (from spacy) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.11/site-packages (from spacy) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.11/site-packages (from spacy) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.11/site-packages (from spacy) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in ./venv/lib/python3.11/site-packages (from spacy) (8.2.1)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./venv/lib/python3.11/site-packages (from spacy) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/lib/python3.11/site-packages (from spacy) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/lib/python3.11/site-packages (from spacy) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./venv/lib/python3.11/site-packages (from spacy) (0.3.2)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./venv/lib/python3.11/site-packages (from spacy) (0.9.0)\r\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./venv/lib/python3.11/site-packages (from spacy) (0.10.2)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/lib/python3.11/site-packages (from spacy) (6.4.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./venv/lib/python3.11/site-packages (from spacy) (2.4.2)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from spacy) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.11/site-packages (from spacy) (65.5.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from spacy) (23.2)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/lib/python3.11/site-packages (from spacy) (3.3.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in ./venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests[socks]->gdown) (3.3.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests[socks]->gdown) (2.0.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests[socks]->gdown) (2023.7.22)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.3)\r\n",
      "Requirement already satisfied: cloudpathlib<0.16.0,>=0.7.0 in ./venv/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.15.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./venv/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown pandas nltk pandarallel spacy autocorrect"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:15:47.813623Z",
     "start_time": "2023-11-30T05:15:44.614362Z"
    }
   },
   "id": "7e13fdc1ec4634c9"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:15:47.823974Z",
     "start_time": "2023-11-30T05:15:47.816403Z"
    }
   },
   "id": "ab3c40973d657442"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download base dataset\n",
    "Dataset is from https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus. Contains over 700000 blog posts with metadata about author and blog"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a15fc659ba8cfb70"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:16:14.140150Z",
     "start_time": "2023-11-30T05:15:47.820566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\r\n",
      "From (uriginal): https://drive.google.com/uc?id=1PJbVYUmRr0_HTwGNtplnu8lG-UCDoXZJ\r\n",
      "From (redirected): https://drive.google.com/uc?id=1PJbVYUmRr0_HTwGNtplnu8lG-UCDoXZJ&confirm=t&uuid=c7ef063b-847a-416c-ad6a-44c3c34bbdd7\r\n",
      "To: /Users/kristian.aars/PycharmProjects/AIAC536-NextWordPrediction/blogtext.csv\r\n",
      "100%|████████████████████████████████████████| 800M/800M [00:23<00:00, 33.6MB/s]\r\n"
     ]
    }
   ],
   "source": [
    "# Download blogtext.csv (700 000 blog posts)\n",
    "!gdown 1PJbVYUmRr0_HTwGNtplnu8lG-UCDoXZJ"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39c2fd6fbd9046ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import from csv-file using Pandas\n",
    "Specify the number of blog posts to process, as the processing can take time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb15f3e001c04e6c"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "N_BLOGPOSTS = 10000\n",
    "\n",
    "df = pd.read_csv('blogtext.csv', nrows=N_BLOGPOSTS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:16:14.241675Z",
     "start_time": "2023-11-30T05:16:14.140358Z"
    }
   },
   "id": "e69fdda6e5caf3c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize sentences\n",
    "This splits all the sentences into arrays, and then explodes the array back to the dataframe as separate sentences. After this, all sentences which are not recognized as strings are removed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8768a4128ba118dd"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "df.text = df.text.transform(lambda t: nltk.sent_tokenize(t))\n",
    "\n",
    "# Explode array into separate rows\n",
    "df = df.explode('text')\n",
    "\n",
    "# Remove all sentences not recognized as strings (numbers, lists etc.)\n",
    "mask = df['text'].apply(lambda x: isinstance(x, str))\n",
    "df = df[mask]\n",
    "\n",
    "# Remove all sentences not containing anything\n",
    "df = df[df['text'] != '']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:16:16.457987Z",
     "start_time": "2023-11-30T05:16:14.257565Z"
    }
   },
   "id": "1a1ca54eb19f5076"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess and clean text\n",
    "This step will clean the text contents and remove unwanted blog-posts from the dataset. Blogposts containing non-english words may be removed and a set percentage of stop-words can be removed to reduce their occurrence in the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14e4994b2a043eb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Specify cleaning parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6986a5cfe5ac73c1"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "remove_stopwords=False\n",
    "rem_stopword_percent=0.00\n",
    "remove_sentence_with_unknown_words=True\n",
    "min_word_count=4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:16:16.465995Z",
     "start_time": "2023-11-30T05:16:16.458657Z"
    }
   },
   "id": "c5ff1c90c5c1af"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import words as en_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:16:16.816528Z",
     "start_time": "2023-11-30T05:16:16.462155Z"
    }
   },
   "id": "4552f1b8adec012e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define some methods for text-cleaning and filtering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa69260ed47c5aa6"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "def contains_non_lexi_word(sentence):\n",
    "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "    lemmatized_doc = spacy_nlp(sentence)\n",
    "    \n",
    "    english_words = en_words.words()\n",
    "    \n",
    "    for token in lemmatized_doc:\n",
    "        word = token.lemma_.lower()\n",
    "        \n",
    "        if word not in english_words:                               \n",
    "            #print(\"Found non-english word {0}\".format(word))\n",
    "            return True\n",
    "        \n",
    "def contains_number(sentence):\n",
    "    return any(char.isdigit() for char in sentence)\n",
    "\n",
    "def word_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:16:16.824272Z",
     "start_time": "2023-11-30T05:16:16.818542Z"
    }
   },
   "id": "2e8281b03a3fd4c2"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "def clean_corpus(corpus_df, rm_contains_unknown_wrd=True, rm_contains_num=True, min_word_count=2, remove_stopwords=True, prob_remove_stopword=0.1):\n",
    "    \n",
    "    # Function which returns wether to keep a sentence in the dataset or not based on given parameters\n",
    "    def keep_sentence(row):\n",
    "        if word_count(row['text']) < min_word_count:\n",
    "            #print(\"{0} is too short\".format(row['text']))\n",
    "            return False\n",
    "        elif rm_contains_num and contains_number(row['text']):\n",
    "            #print(\"{0} contains number\".format(row['text']))\n",
    "            return False\n",
    "        elif rm_contains_unknown_wrd and contains_non_lexi_word(row['text']):\n",
    "            #print(\"{0} contains unknown words\".format(row['text']))\n",
    "            return False\n",
    "        else:\n",
    "            #print(\"{0} is ok\".format(row['text']))\n",
    "            return True\n",
    "    \n",
    "    # Function to normalize and clean the text\n",
    "    def clean_text(text):\n",
    "        # Convert to lowercase for better normalization\n",
    "        text = text.lower()\n",
    "    \n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "        # Tokenize the words\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # Remove stop words\n",
    "        if remove_stopwords:\n",
    "            if random.random() < prob_remove_stopword:\n",
    "                words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # Join the cleaned words back into a sentence\n",
    "        cleaned_text = ' '.join(words)\n",
    "    \n",
    "        return cleaned_text\n",
    "\n",
    "    pre_rem_size = corpus_df.shape[0]\n",
    "    pandarallel.initialize()\n",
    "    corpus_df['text'] = corpus_df['text'].apply(clean_text)\n",
    "    corpus_df = corpus_df[corpus_df.parallel_apply(keep_sentence, axis=1)]\n",
    "    sen_removed = pre_rem_size - corpus_df.shape[0]\n",
    "    print(\"Removed {0} sentences because they did not conform to the sentence specifications\".format(sen_removed))\n",
    "    \n",
    "    return corpus_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:16:16.831269Z",
     "start_time": "2023-11-30T05:16:16.824945Z"
    }
   },
   "id": "437e691fd1384733"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let the cleaning begin\n",
    "This might take some while. Please be patient. 5000 blog posts takes around 10 minutes on a M1 Pro MacBook Pro 14\" laptop. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6e51927426dc5d9"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "Removed 72313 sentences because they did not conform to the sentence specifications\n"
     ]
    }
   ],
   "source": [
    "df = clean_corpus(df, remove_stopwords=remove_stopwords, rm_contains_unknown_wrd=remove_sentence_with_unknown_words, prob_remove_stopword=rem_stopword_percent, min_word_count=min_word_count)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:35:14.789794Z",
     "start_time": "2023-11-30T05:16:16.828145Z"
    }
   },
   "id": "bc27a77b0503c2aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lastly, save the file so it can be loaded for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "860ae26913074e40"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "df.to_csv(\"blogtext_cleaned.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:43:08.530123Z",
     "start_time": "2023-11-30T05:43:08.426658Z"
    }
   },
   "id": "ccc788b969487ded"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f9706563a0bb0f06"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
