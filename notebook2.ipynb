{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T06:56:08.777616Z",
     "start_time": "2023-10-13T06:56:07.738652Z"
    }
   },
   "id": "f95d79b0e4965d34"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:03:14.140535Z",
     "start_time": "2023-10-13T07:03:05.763532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.41 s, sys: 657 ms, total: 8.07 s\n",
      "Wall time: 8.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Import dataset from CSV\n",
    "\n",
    "df = pd.read_csv('blogtext.csv').head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": "        id gender  age              topic      sign          date  \\\n0  2059027   male   15            Student       Leo   14,May,2004   \n1  2059027   male   15            Student       Leo   13,May,2004   \n2  2059027   male   15            Student       Leo   12,May,2004   \n3  2059027   male   15            Student       Leo   12,May,2004   \n4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n5  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n6  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n7  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n8  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n9  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n\n                                                text  \n0             Info has been found (+/- 100 pages,...  \n1             These are the team members:   Drewe...  \n2             In het kader van kernfusie op aarde...  \n3                   testing!!!  testing!!!            \n4               Thanks to Yahoo!'s Toolbar I can ...  \n5               I had an interesting conversation...  \n6               Somehow Coca-Cola has a way of su...  \n7               If anything, Korea is a country o...  \n8               Take a read of this news article ...  \n9               I surf the English news sites a l...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>topic</th>\n      <th>sign</th>\n      <th>date</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>14,May,2004</td>\n      <td>Info has been found (+/- 100 pages,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>13,May,2004</td>\n      <td>These are the team members:   Drewe...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>12,May,2004</td>\n      <td>In het kader van kernfusie op aarde...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>12,May,2004</td>\n      <td>testing!!!  testing!!!</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>11,June,2004</td>\n      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>I had an interesting conversation...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>Somehow Coca-Cola has a way of su...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>If anything, Korea is a country o...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>Take a read of this news article ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>09,June,2004</td>\n      <td>I surf the English news sites a l...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:03:14.144074Z",
     "start_time": "2023-10-13T07:03:14.140091Z"
    }
   },
   "id": "377b4d2ee44e3e05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d565cedce4c8d393"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.8 ms, sys: 2.72 ms, total: 56.5 ms\n",
      "Wall time: 55.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.text = df.text.transform(lambda t: nltk.sent_tokenize(t))\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:03:14.262910Z",
     "start_time": "2023-10-13T07:03:14.191510Z"
    }
   },
   "id": "6450521ad0458ab3"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(2168, 7)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.explode('text')\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:03:27.760221Z",
     "start_time": "2023-10-13T07:03:27.751828Z"
    }
   },
   "id": "35e293b7db4fb277"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#df.text.to_csv(\"blogtext-sentence_tokenized.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T12:43:37.515580Z",
     "start_time": "2023-10-11T12:43:37.217552Z"
    }
   },
   "id": "54052f0d1e6589d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepearing data for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aa0d2bb6fe1f060"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from keras.src.preprocessing.text import Tokenizer\n",
    "from keras.src.utils import pad_sequences, to_categorical\n",
    "\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "\n",
    "import spacy\n",
    "from autocorrect import Speller\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:03:31.272818Z",
     "start_time": "2023-10-13T07:03:30.741581Z"
    }
   },
   "id": "a8accc7b14d2daeb"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def has_url(sentence):\n",
    "    doc = spacy_nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.like_url:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def has_email(sentence):\n",
    "    doc = spacy_nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.like_email:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_phonenumber(sentence):\n",
    "    doc = spacy_nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.like_num:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def autocorrect_corpus(corpus):\n",
    "    speller = Speller(lang='en')\n",
    "    return corpus.transform(lambda s: speller(s['text']))\n",
    "\n",
    "def clean_corpus(corpus_df, rm_sentence_phone=True, rm_sentence_email=True, rm_sentence_url=True):\n",
    "    # Remove sentences with personal details as specified by function parameters\n",
    "    def remove_sentences_condition(row):\n",
    "        if rm_sentence_phone and has_phonenumber(row['text']): return False\n",
    "        elif rm_sentence_email and has_email(row['text']): return False\n",
    "        elif rm_sentence_url and has_url(row['text']): return False\n",
    "        else: return True\n",
    "    \n",
    "    pre_rem_size = corpus_df.shape[0]\n",
    "    corpus_df = corpus_df[corpus_df.apply(remove_sentences_condition, axis=1)]\n",
    "    sen_removed = pre_rem_size - corpus_df.shape[0]\n",
    "    print(\"Removed {0} sentences because they contained email, phone, or url(s)\".format(sen_removed))\n",
    "    \n",
    "    # Run autocorrect to fix text-typos\n",
    "    autocorrect_corpus(corpus_df)\n",
    "    \n",
    "    # En løsning er å fjerne alle ord som ikke eksisterer i det engelske vokabularet.\n",
    "    \n",
    "    return corpus_df\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:06:44.166707Z",
     "start_time": "2023-10-13T07:06:44.152249Z"
    }
   },
   "id": "cb6ef90035ef7065"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 sentences because they contained email, phone, or url(s)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3789\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3790\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3791\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:152\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:160\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/index_class_helper.pxi:70\u001B[0m, in \u001B[0;36mpandas._libs.index.Int64Engine._check_type\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/apply.py:299\u001B[0m, in \u001B[0;36mApply.transform_str_or_callable\u001B[0;34m(self, func)\u001B[0m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/frame.py:10037\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m  10027\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[1;32m  10028\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m  10029\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  10035\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[1;32m  10036\u001B[0m )\n\u001B[0;32m> 10037\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/apply.py:837\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    835\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw()\n\u001B[0;32m--> 837\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/apply.py:963\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    962\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 963\u001B[0m     results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    965\u001B[0m     \u001B[38;5;66;03m# wrap results\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/apply.py:979\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    977\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[1;32m    978\u001B[0m     \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[0;32m--> 979\u001B[0m     results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    980\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[1;32m    981\u001B[0m         \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[1;32m    982\u001B[0m         \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[32], line 24\u001B[0m, in \u001B[0;36mautocorrect_corpus.<locals>.<lambda>\u001B[0;34m(s)\u001B[0m\n\u001B[1;32m     23\u001B[0m speller \u001B[38;5;241m=\u001B[39m Speller(lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m corpus\u001B[38;5;241m.\u001B[39mtransform(\u001B[38;5;28;01mlambda\u001B[39;00m s: speller(\u001B[43ms\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m))\n",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/series.py:1040\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[0;32m-> 1040\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1042\u001B[0m \u001B[38;5;66;03m# Convert generator to list before going through hashable part\u001B[39;00m\n\u001B[1;32m   1043\u001B[0m \u001B[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/series.py:1156\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[0;34m(self, label, takeable)\u001B[0m\n\u001B[1;32m   1155\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[0;32m-> 1156\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(loc):\n",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3796\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3797\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3798\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3799\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3800\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3801\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mclean_corpus\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrm_sentence_phone\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrm_sentence_url\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrm_sentence_email\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[32], line 40\u001B[0m, in \u001B[0;36mclean_corpus\u001B[0;34m(corpus_df, rm_sentence_phone, rm_sentence_email, rm_sentence_url)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRemoved \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m sentences because they contained email, phone, or url(s)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(sen_removed))\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Run autocorrect to fix text-typos\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m \u001B[43mautocorrect_corpus\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m corpus_df\n",
      "Cell \u001B[0;32mIn[32], line 24\u001B[0m, in \u001B[0;36mautocorrect_corpus\u001B[0;34m(corpus)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mautocorrect_corpus\u001B[39m(corpus):\n\u001B[1;32m     23\u001B[0m     speller \u001B[38;5;241m=\u001B[39m Speller(lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcorpus\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mspeller\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/frame.py:9863\u001B[0m, in \u001B[0;36mDataFrame.transform\u001B[0;34m(self, func, axis, *args, **kwargs)\u001B[0m\n\u001B[1;32m   9860\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[1;32m   9862\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\u001B[38;5;28mself\u001B[39m, func\u001B[38;5;241m=\u001B[39mfunc, axis\u001B[38;5;241m=\u001B[39maxis, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m-> 9863\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   9864\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, DataFrame)\n\u001B[1;32m   9865\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/apply.py:231\u001B[0m, in \u001B[0;36mApply.transform\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    229\u001B[0m func \u001B[38;5;241m=\u001B[39m cast(AggFuncTypeBase, func)\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 231\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform_str_or_callable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/pandas/core/apply.py:301\u001B[0m, in \u001B[0;36mApply.transform_str_or_callable\u001B[0;34m(self, func)\u001B[0m\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39mapply(func, args\u001B[38;5;241m=\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m--> 301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[32], line 24\u001B[0m, in \u001B[0;36mautocorrect_corpus.<locals>.<lambda>\u001B[0;34m(s)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mautocorrect_corpus\u001B[39m(corpus):\n\u001B[1;32m     23\u001B[0m     speller \u001B[38;5;241m=\u001B[39m Speller(lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m corpus\u001B[38;5;241m.\u001B[39mtransform(\u001B[38;5;28;01mlambda\u001B[39;00m s: \u001B[43mspeller\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/autocorrect/__init__.py:128\u001B[0m, in \u001B[0;36mSpeller.autocorrect_sentence\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mautocorrect_sentence\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence):\n\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    129\u001B[0m \u001B[43m        \u001B[49m\u001B[43mword_regexes\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlang\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    130\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmatch\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautocorrect_word\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    131\u001B[0m \u001B[43m        \u001B[49m\u001B[43msentence\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    132\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/re/__init__.py:185\u001B[0m, in \u001B[0;36msub\u001B[0;34m(pattern, repl, string, count, flags)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msub\u001B[39m(pattern, repl, string, count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m    179\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001B[39;00m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001B[39;00m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001B[39;00m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001B[39;00m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;124;03m    a callable, it's passed the Match object and must return\u001B[39;00m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;124;03m    a replacement string to be used.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstring\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcount\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: expected string or bytes-like object, got 'Series'"
     ]
    }
   ],
   "source": [
    "df = clean_corpus(df, rm_sentence_phone=False, rm_sentence_url=False, rm_sentence_email=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:09:10.378827Z",
     "start_time": "2023-10-13T07:07:43.054491Z"
    }
   },
   "id": "7284a44ceb21d9ee"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "2168"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df['text']\n",
    "\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T07:04:31.793775Z",
     "start_time": "2023-10-13T07:04:31.782145Z"
    }
   },
   "id": "b6df1f532b86fec3"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0               Info has been found (+/- 100 pages,...\n1               These are the team members:   Drewe...\n2               In het kader van kernfusie op aarde...\n2    Date: 7 Feb 1994 07:41:14 GMT Organization: Th...\n2    Seemed to be a transcript of a 'Seven Days' ar...\n                           ...                        \n2    Repeat this step until you have the required 1...\n2    (Safety note: Don't put all your enriched uran...\n2    Use at least two or three buckets and keep the...\n2    This will prevent the premature build-up of a ...\n2    Now it's time to convert your enriched uranium...\nName: text, Length: 100, dtype: object"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T12:43:48.531874Z",
     "start_time": "2023-10-11T12:43:48.524417Z"
    }
   },
   "id": "770b33009bff8548"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "sentence_list = df.tolist()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence_list)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T12:43:49.470983Z",
     "start_time": "2023-10-11T12:43:49.469Z"
    }
   },
   "id": "52eab5627e64f910"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "6369"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T12:43:50.378644Z",
     "start_time": "2023-10-11T12:43:50.366003Z"
    }
   },
   "id": "11d1f4229e029de6"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{'the': 1,\n 'a': 2,\n 'to': 3,\n 'and': 4,\n 'of': 5,\n 'i': 6,\n 'in': 7,\n 'is': 8,\n 'it': 9,\n 'that': 10,\n 'for': 11,\n 'you': 12,\n 'urllink': 13,\n 'but': 14,\n 'on': 15,\n 'my': 16,\n 'was': 17,\n 'as': 18,\n 'this': 19,\n 'have': 20,\n 'are': 21,\n 'or': 22,\n 'be': 23,\n 'not': 24,\n 'so': 25,\n 'they': 26,\n 'here': 27,\n 'one': 28,\n 'at': 29,\n 'from': 30,\n 'with': 31,\n 'there': 32,\n 'if': 33,\n 'like': 34,\n 'he': 35,\n 'korean': 36,\n \"it's\": 37,\n 'we': 38,\n 'can': 39,\n 'about': 40,\n 'all': 41,\n 'get': 42,\n 'me': 43,\n 'some': 44,\n 'do': 45,\n 'out': 46,\n 'korea': 47,\n 'just': 48,\n 'what': 49,\n 'when': 50,\n 'up': 51,\n 'know': 52,\n 'your': 53,\n 'well': 54,\n 'no': 55,\n 'more': 56,\n 'will': 57,\n 'their': 58,\n 'now': 59,\n 'an': 60,\n 'then': 61,\n 'has': 62,\n 'who': 63,\n 'had': 64,\n 'were': 65,\n 'time': 66,\n 'seoul': 67,\n 'them': 68,\n 'which': 69,\n 'how': 70,\n 'many': 71,\n 'by': 72,\n 'little': 73,\n 'go': 74,\n 'pretty': 75,\n 'think': 76,\n 'take': 77,\n 'good': 78,\n '2': 79,\n 'place': 80,\n \"i'm\": 81,\n 'koreans': 82,\n 'would': 83,\n 'too': 84,\n 'see': 85,\n 'into': 86,\n 'much': 87,\n 'people': 88,\n 'few': 89,\n 'things': 90,\n \"'\": 91,\n 'really': 92,\n 'bomb': 93,\n 'even': 94,\n 'only': 95,\n 'may': 96,\n 'also': 97,\n 'her': 98,\n 'his': 99,\n 'another': 100,\n '3': 101,\n '1': 102,\n 'than': 103,\n 'where': 104,\n 'him': 105,\n 'make': 106,\n 'other': 107,\n 'back': 108,\n 'these': 109,\n 'look': 110,\n 'u': 111,\n 'first': 112,\n 'after': 113,\n \"don't\": 114,\n 'uranium': 115,\n 'got': 116,\n 'home': 117,\n 'why': 118,\n 'way': 119,\n 'thing': 120,\n '000': 121,\n 'said': 122,\n 'been': 123,\n '5': 124,\n 'did': 125,\n 'never': 126,\n 'our': 127,\n 'say': 128,\n 'life': 129,\n 'year': 130,\n 'last': 131,\n 'us': 132,\n 'day': 133,\n 'she': 134,\n 'two': 135,\n \"that's\": 136,\n 'work': 137,\n 'guy': 138,\n 'could': 139,\n 'because': 140,\n 'canada': 141,\n 'went': 142,\n 'still': 143,\n 'those': 144,\n 'though': 145,\n 'over': 146,\n 'again': 147,\n 'lot': 148,\n 'actually': 149,\n 'most': 150,\n 'almost': 151,\n 'off': 152,\n 'new': 153,\n 'course': 154,\n 'same': 155,\n 'before': 156,\n 'years': 157,\n 'love': 158,\n 'any': 159,\n 'something': 160,\n 'old': 161,\n \"you're\": 162,\n 'every': 163,\n 'days': 164,\n 's': 165,\n 'right': 166,\n 'kids': 167,\n 'going': 168,\n 'bit': 169,\n 'sure': 170,\n '10': 171,\n 'put': 172,\n 'friend': 173,\n 'especially': 174,\n 'seems': 175,\n \"didn't\": 176,\n 'h': 177,\n 'least': 178,\n 'should': 179,\n 'everything': 180,\n 'interesting': 181,\n \"i've\": 182,\n 'next': 183,\n 'always': 184,\n 'ok': 185,\n 'english': 186,\n 'vancouver': 187,\n 'am': 188,\n 'real': 189,\n \"you'll\": 190,\n 'its': 191,\n '20': 192,\n 'best': 193,\n 'very': 194,\n 'such': 195,\n 'anyways': 196,\n 'night': 197,\n 'came': 198,\n 'part': 199,\n 'need': 200,\n 'times': 201,\n 'long': 202,\n 'oh': 203,\n 'wife': 204,\n 'called': 205,\n 'find': 206,\n 'once': 207,\n 'while': 208,\n \"can't\": 209,\n 'quite': 210,\n 'reason': 211,\n 'business': 212,\n 'different': 213,\n 'guys': 214,\n 'down': 215,\n 'each': 216,\n 'through': 217,\n 'around': 218,\n 'idea': 219,\n 'cool': 220,\n 'although': 221,\n 'sometimes': 222,\n 'bar': 223,\n 'i’m': 224,\n 'war': 225,\n 'used': 226,\n 'getting': 227,\n 'won': 228,\n 'maybe': 229,\n 'yeouido': 230,\n 'mother': 231,\n 'friends': 232,\n 'want': 233,\n 'made': 234,\n 'morning': 235,\n 'remember': 236,\n 'saw': 237,\n 'being': 238,\n 'men': 239,\n 'man': 240,\n 'nuclear': 241,\n 'enough': 242,\n 'use': 243,\n 'city': 244,\n 'world': 245,\n 'ones': 246,\n 'makes': 247,\n 'means': 248,\n 'everyone': 249,\n 'seem': 250,\n 'article': 251,\n 'food': 252,\n 'might': 253,\n 'without': 254,\n 'area': 255,\n 'sense': 256,\n 'north': 257,\n 'show': 258,\n 'country': 259,\n 'guess': 260,\n 'story': 261,\n 'usa': 262,\n '4': 263,\n 'places': 264,\n 'usually': 265,\n 'both': 266,\n 'goes': 267,\n 'money': 268,\n 'school': 269,\n 'movie': 270,\n 'read': 271,\n 'call': 272,\n 'below': 273,\n 'myself': 274,\n 'western': 275,\n 'own': 276,\n 'kind': 277,\n 'tell': 278,\n 'probably': 279,\n 'usd': 280,\n 'anything': 281,\n 'great': 282,\n 'blog': 283,\n 'building': 284,\n 'pic': 285,\n 'together': 286,\n 'south': 287,\n 'better': 288,\n 'yourself': 289,\n 'left': 290,\n 'couple': 291,\n 'does': 292,\n 'government': 293,\n 'high': 294,\n 'wonder': 295,\n 'end': 296,\n 'park': 297,\n 'however': 298,\n 'name': 299,\n 'gay': 300,\n \"he's\": 301,\n 'girls': 302,\n 'foreign': 303,\n 'away': 304,\n '235': 305,\n 'already': 306,\n 'hand': 307,\n 'family': 308,\n 'seen': 309,\n 'bad': 310,\n 'thought': 311,\n 'foreigners': 312,\n 'live': 313,\n 'phone': 314,\n \"i'll\": 315,\n '7': 316,\n 'university': 317,\n 'full': 318,\n 'three': 319,\n '50': 320,\n 'until': 321,\n 'movies': 322,\n 'mean': 323,\n 'local': 324,\n 'thus': 325,\n 'stop': 326,\n 'let': 327,\n 'alone': 328,\n 'face': 329,\n 'near': 330,\n 'nice': 331,\n '100': 332,\n 'bombs': 333,\n 'matter': 334,\n 'less': 335,\n 'easy': 336,\n 'hard': 337,\n 'street': 338,\n 'minutes': 339,\n 'living': 340,\n 'saying': 341,\n '6': 342,\n 'thinking': 343,\n 'come': 344,\n 'president': 345,\n 'road': 346,\n 'language': 347,\n 'someone': 348,\n 'looking': 349,\n 'looks': 350,\n 'mine': 351,\n 'women': 352,\n 'perfect': 353,\n 'ya': 354,\n 'pics': 355,\n 'subway': 356,\n 'found': 357,\n 'big': 358,\n 'taking': 359,\n 'simple': 360,\n 'plutonium': 361,\n 'air': 362,\n 'buy': 363,\n 'done': 364,\n 'forget': 365,\n 'fast': 366,\n 'keep': 367,\n 'figure': 368,\n \"they're\": 369,\n 'trying': 370,\n 'believe': 371,\n \"isn't\": 372,\n 'feel': 373,\n 'version': 374,\n 'basically': 375,\n 'boy': 376,\n 'west': 377,\n 'town': 378,\n 'week': 379,\n 'gal': 380,\n 'heard': 381,\n 'today': 382,\n 'lovely': 383,\n 'happen': 384,\n 'trip': 385,\n 'gals': 386,\n 'miss': 387,\n \"there's\": 388,\n 'comes': 389,\n 'beer': 390,\n 'hot': 391,\n 'it’s': 392,\n 'themselves': 393,\n 'news': 394,\n \"you've\": 395,\n 'likely': 396,\n 'san': 397,\n 'asked': 398,\n 'firms': 399,\n 'top': 400,\n 'white': 401,\n 'similar': 402,\n 'open': 403,\n 'soon': 404,\n 'ever': 405,\n '9': 406,\n 'started': 407,\n 'change': 408,\n 'drinks': 409,\n 'yes': 410,\n 'hours': 411,\n 'p': 412,\n 'picture': 413,\n 'fat': 414,\n 'doing': 415,\n 'changed': 416,\n 'wanted': 417,\n 'months': 418,\n 'pork': 419,\n 'parents': 420,\n 'office': 421,\n 'memory': 422,\n 'between': 423,\n 'under': 424,\n '11': 425,\n 'true': 426,\n 'start': 427,\n 'enriched': 428,\n 'store': 429,\n 'floor': 430,\n 'whole': 431,\n 'above': 432,\n 'stuff': 433,\n 'girl': 434,\n 'since': 435,\n 'group': 436,\n '60': 437,\n 'free': 438,\n 'fall': 439,\n 'else': 440,\n 'took': 441,\n \"i'd\": 442,\n 'special': 443,\n 'says': 444,\n 'dog': 445,\n 'ice': 446,\n 'later': 447,\n 'literally': 448,\n 'older': 449,\n 'o': 450,\n 'having': 451,\n 'airport': 452,\n 'funny': 453,\n 'scene': 454,\n 'dong': 455,\n 'walk': 456,\n 'party': 457,\n 'eyes': 458,\n 'anyone': 459,\n 'please': 460,\n 'general': 461,\n 'heart': 462,\n 'give': 463,\n 'typical': 464,\n 'nothing': 465,\n 'room': 466,\n 'public': 467,\n 'eat': 468,\n 'site': 469,\n 'side': 470,\n 'instead': 471,\n 'point': 472,\n 'market': 473,\n 'across': 474,\n 'families': 475,\n \"let's\": 476,\n \"'the\": 477,\n 'fact': 478,\n 'often': 479,\n 'estate': 480,\n 'heck': 481,\n 'system': 482,\n 'apparently': 483,\n 'canadian': 484,\n 'met': 485,\n 'restaurant': 486,\n 'drink': 487,\n 'leave': 488,\n 'fair': 489,\n 'god': 490,\n 'care': 491,\n 'children': 492,\n 'perhaps': 493,\n 'bus': 494,\n 'major': 495,\n 'china': 496,\n 'ro': 497,\n 'young': 498,\n 'mind': 499,\n 'making': 500,\n 'step': 501,\n 'view': 502,\n 'countries': 503,\n 'set': 504,\n 'e': 505,\n \"won't\": 506,\n 'plants': 507,\n 'form': 508,\n 'small': 509,\n 'able': 510,\n 'ways': 511,\n 'lead': 512,\n 'inside': 513,\n 'second': 514,\n 'yet': 515,\n 'help': 516,\n 'far': 517,\n 'class': 518,\n 'economic': 519,\n 'million': 520,\n 'death': 521,\n 'others': 522,\n '8': 523,\n 'age': 524,\n 'move': 525,\n 'brutal': 526,\n \"wasn't\": 527,\n 'district': 528,\n '30': 529,\n 'gyup': 530,\n 'sal': 531,\n 'sleep': 532,\n 'law': 533,\n 't': 534,\n 'must': 535,\n 'car': 536,\n 'front': 537,\n 'cheap': 538,\n 'summer': 539,\n 'sinchon': 540,\n 'carlo': 541,\n 'future': 542,\n 'writing': 543,\n 'don’t': 544,\n 'decided': 545,\n 'level': 546,\n 'apart': 547,\n 'energy': 548,\n 'order': 549,\n 'pounds': 550,\n 'ready': 551,\n 'plant': 552,\n 'finally': 553,\n 'known': 554,\n 'cells': 555,\n 'weeks': 556,\n 'short': 557,\n 'happens': 558,\n 'door': 559,\n 'gets': 560,\n 'owner': 561,\n 'takes': 562,\n 'longer': 563,\n 'dad': 564,\n 'investment': 565,\n 'save': 566,\n 'ended': 567,\n 'culture': 568,\n 'river': 569,\n 'joongang': 570,\n 'iraq': 571,\n 'sister': 572,\n 'child': 573,\n 'learn': 574,\n 'bud': 575,\n 'son': 576,\n 'knew': 577,\n 'speak': 578,\n 'lots': 579,\n 'deal': 580,\n 'hongdae': 581,\n 'single': 582,\n 'lg': 583,\n 'pictures': 584,\n 'taken': 585,\n 'told': 586,\n 'bathroom': 587,\n 'crazy': 588,\n 'service': 589,\n 'educated': 590,\n 'spent': 591,\n 'weird': 592,\n 'job': 593,\n 'daehak': 594,\n 'walking': 595,\n \"here's\": 596,\n 'shot': 597,\n 'political': 598,\n 'sock': 599,\n 'net': 600,\n 'fun': 601,\n 'low': 602,\n 'radiation': 603,\n 'according': 604,\n 'percent': 605,\n 'sounds': 606,\n 'coming': 607,\n 'reading': 608,\n 'table': 609,\n 'note': 610,\n 'hexafluoride': 611,\n 'cover': 612,\n 'head': 613,\n 'easily': 614,\n 'red': 615,\n 'early': 616,\n 'number': 617,\n 'upon': 618,\n 'problem': 619,\n 'bowls': 620,\n 'bowl': 621,\n 'body': 622,\n 'truly': 623,\n 'drive': 624,\n 'example': 625,\n 'word': 626,\n 'c': 627,\n 'lithium': 628,\n 'answer': 629,\n 'question': 630,\n 'large': 631,\n 'shopping': 632,\n 'within': 633,\n 'running': 634,\n 'simply': 635,\n 'largest': 636,\n '24': 637,\n 'latest': 638,\n 'lady': 639,\n 'lost': 640,\n 'person': 641,\n 'alright': 642,\n 'imagine': 643,\n 'lately': 644,\n 'buildings': 645,\n 'han': 646,\n '500': 647,\n 'yesterday': 648,\n 'bush': 649,\n 'seeing': 650,\n 'list': 651,\n 'married': 652,\n 'himself': 653,\n 'visit': 654,\n 'stay': 655,\n 'company': 656,\n 'finance': 657,\n 'named': 658,\n 'ago': 659,\n 'bars': 660,\n 'fine': 661,\n 'inline': 662,\n 'camera': 663,\n \"couldn't\": 664,\n 'hate': 665,\n 'price': 666,\n 'learned': 667,\n 'layers': 668,\n 'super': 669,\n 'younger': 670,\n 'father': 671,\n 'clean': 672,\n 'dirty': 673,\n 'police': 674,\n '18': 675,\n 'everyday': 676,\n 'discount': 677,\n 'line': 678,\n 'nights': 679,\n 'busy': 680,\n 'hey': 681,\n 'noodles': 682,\n 'recent': 683,\n 'traffic': 684,\n 'book': 685,\n 'connection': 686,\n 'fire': 687,\n 'close': 688,\n 'theatre': 689,\n 'words': 690,\n 'driving': 691,\n 'art': 692,\n 'card': 693,\n 'topic': 694,\n 'boss': 695,\n 'mermaid': 696,\n 'moore': 697,\n 'dance': 698,\n 'mountain': 699,\n 'images': 700,\n 'he’d': 701,\n 'ring': 702,\n 'australia': 703,\n 'http': 704,\n 'wants': 705,\n 'atomic': 706,\n 'given': 707,\n 'american': 708,\n 'chinese': 709,\n 'amount': 710,\n 'mark': 711,\n '238': 712,\n '40': 713,\n 'united': 714,\n 'washington': 715,\n 'truck': 716,\n 'convenience': 717,\n 'hold': 718,\n 'commercial': 719,\n 'ii': 720,\n 'process': 721,\n 'bucket': 722,\n 'handle': 723,\n 'cream': 724,\n 'stores': 725,\n 'avoid': 726,\n 'working': 727,\n 'depending': 728,\n 'case': 729,\n 'following': 730,\n 'sam': 731,\n 'feeling': 732,\n 'degrees': 733,\n 'four': 734,\n 'welfare': 735,\n \"they'd\": 736,\n 'rice': 737,\n \"doesn't\": 738,\n 'earth': 739,\n 'completely': 740,\n 'skin': 741,\n 'thanks': 742,\n 'pop': 743,\n 'click': 744,\n 'hour': 745,\n 'account': 746,\n 'paid': 747,\n 'prices': 748,\n 'chung': 749,\n 'amazing': 750,\n 'everywhere': 751,\n 'died': 752,\n 'ilbo': 753,\n 'state': 754,\n 'run': 755,\n 'ground': 756,\n 'policy': 757,\n 'moving': 758,\n 'check': 759,\n 'support': 760,\n 'ah': 761,\n 'figured': 762,\n 'wall': 763,\n 'outside': 764,\n 'seller': 765,\n 'army': 766,\n 'soju': 767,\n 'eh': 768,\n 'itaewon': 769,\n 'pick': 770,\n 'knows': 771,\n 'speaking': 772,\n 'pool': 773,\n 'marry': 774,\n 'pay': 775,\n 'study': 776,\n 'strong': 777,\n 'complete': 778,\n 'swimming': 779,\n 'cold': 780,\n 'ask': 781,\n 'shows': 782,\n 'economy': 783,\n 'past': 784,\n 'compared': 785,\n 'noticed': 786,\n 'served': 787,\n 'paying': 788,\n 'firm': 789,\n 'companies': 790,\n 'unless': 791,\n 'germany': 792,\n 'greece': 793,\n 'hands': 794,\n 'explain': 795,\n 'knowing': 796,\n 'mart': 797,\n 'either': 798,\n 'station': 799,\n 'um': 800,\n 'suicide': 801,\n 'odd': 802,\n 'dancing': 803,\n '46041': 804,\n 'weekend': 805,\n 'bye': 806,\n 'we’re': 807,\n 'broke': 808,\n 'team': 809,\n 'build': 810,\n 'seemed': 811,\n '0': 812,\n 'americans': 813,\n 'victim': 814,\n 'training': 815,\n \"we're\": 816,\n 'enjoy': 817,\n 'worse': 818,\n 'along': 819,\n 'security': 820,\n 'rest': 821,\n 'nucleus': 822,\n '90': 823,\n 'conversion': 824,\n 'turn': 825,\n 'metal': 826,\n 'v': 827,\n 'york': 828,\n 'coffee': 829,\n 'type': 830,\n 'available': 831,\n 'supply': 832,\n 'house': 833,\n 'pound': 834,\n 'pain': 835,\n 'difficult': 836,\n 'cost': 837,\n 'easier': 838,\n 'foot': 839,\n 'possible': 840,\n 'smaller': 841,\n 'larger': 842,\n 'five': 843,\n 'wondering': 844,\n 'winter': 845,\n 'finished': 846,\n 'play': 847,\n 'spend': 848,\n 'relatively': 849,\n 'sales': 850,\n 'chance': 851,\n 'cash': 852,\n 'expected': 853,\n 'experience': 854,\n 'tax': 855,\n 'mention': 856,\n 'whether': 857,\n 'try': 858,\n 'conversation': 859,\n 'video': 860,\n 'tried': 861,\n 'truth': 862,\n 'facts': 863,\n 'survive': 864,\n 'weather': 865,\n 'types': 866,\n 'heads': 867,\n 'forever': 868,\n 'hope': 869,\n \"'s\": 870,\n 'link': 871,\n 'stock': 872,\n 'during': 873,\n 'tv': 874,\n 'comment': 875,\n 'rich': 876,\n 'important': 877,\n 'apartment': 878,\n 'rent': 879,\n '200': 880,\n 'returned': 881,\n 'theme': 882,\n 'based': 883,\n \"korea's\": 884,\n 'hee': 885,\n 'dictator': 886,\n '1979': 887,\n 'japan': 888,\n 'games': 889,\n 'online': 890,\n 'troops': 891,\n 'current': 892,\n 'forces': 893,\n 'section': 894,\n 'unreal': 895,\n 'republic': 896,\n 'wrong': 897,\n 'starts': 898,\n 'sentence': 899,\n 'hear': 900,\n \"wife's\": 901,\n 'dynasty': 902,\n 'email': 903,\n 'ceo': 904,\n 'meeting': 905,\n 'spoke': 906,\n 'works': 907,\n 'needed': 908,\n 'actual': 909,\n 'showed': 910,\n 'sign': 911,\n 'bought': 912,\n 'bags': 913,\n 'favorite': 914,\n 'rollerblading': 915,\n 'j': 916,\n 'ladies': 917,\n 'stand': 918,\n 'spencer': 919,\n \"aren't\": 920,\n 'warm': 921,\n 'jetlag': 922,\n 'kinda': 923,\n 'beach': 924,\n 'kim': 925,\n 'stations': 926,\n 'military': 927,\n 'husband': 928,\n 'behind': 929,\n \"'i'm\": 930,\n \"he'd\": 931,\n 'moment': 932,\n 'control': 933,\n 'broken': 934,\n 'meet': 935,\n 'pepper': 936,\n 'island': 937,\n 'pusan': 938,\n 'restaurants': 939,\n 'sorry': 940,\n '12': 941,\n '16': 942,\n '31': 943,\n 'india': 944,\n \"it'll\": 945,\n 'scary': 946,\n 'looked': 947,\n 'land': 948,\n 'spaghetti': 949,\n 'wild': 950,\n 'music': 951,\n 'middle': 952,\n 'become': 953,\n 'despite': 954,\n 'dreams': 955,\n 'dream': 956,\n 'nbsp': 957,\n 'sent': 958,\n 'record': 959,\n 'light': 960,\n 'bricx': 961,\n 'king': 962,\n 'moves': 963,\n 'wings': 964,\n 'squid': 965,\n 'holding': 966,\n 'entire': 967,\n 'reality': 968,\n 'can’t': 969,\n '”': 970,\n 'www': 971,\n 'uploadimages': 972,\n 'jpg': 973,\n 'wait': 974,\n 'html': 975,\n 'van': 976,\n 'humor': 977,\n 'gold': 978,\n 'push': 979,\n 'charge': 980,\n 'chicken': 981,\n 'kitchen': 982,\n 'die': 983,\n 'familiar': 984,\n 'rather': 985,\n 'french': 986,\n 'thank': 987,\n 'national': 988,\n 'nuclei': 989,\n 'kinds': 990,\n 'ten': 991,\n 'critical': 992,\n 'mass': 993,\n 'steal': 994,\n 'gaseous': 995,\n 'oxide': 996,\n 'former': 997,\n 'services': 998,\n 'steel': 999,\n 'grade': 1000,\n ...}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T12:43:50.889268Z",
     "start_time": "2023-10-11T12:43:50.880554Z"
    }
   },
   "id": "ff31f871b9c4f0e9"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "n_gram_list = []\n",
    "n_gram_length = 5\n",
    "\n",
    "for line in sentence_list:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    for n in range(2, n_gram_length):\n",
    "        n_grams = ngrams(token_list, n)\n",
    "        n_gram_list.extend(np.asarray([*n_grams]))\n",
    "\n",
    "# Padding\n",
    "n_gram_list = np.array(pad_sequences(\n",
    "    n_gram_list,\n",
    "    maxlen=n_gram_length,\n",
    "    padding='pre'\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T13:03:36.171246Z",
     "start_time": "2023-10-11T13:03:36.030800Z"
    }
   },
   "id": "a636c919dc7fcba1"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "X = n_gram_list[:, :-1]\n",
    "y = n_gram_list[:, -1]\n",
    "\n",
    "y = to_categorical(y, num_classes=total_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T13:03:38.031569Z",
     "start_time": "2023-10-11T13:03:37.695192Z"
    }
   },
   "id": "b254b0558e1de286"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 4, 10)             63690     \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 4, 128)            53760     \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6369)              821601    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1054635 (4.02 MB)\n",
      "Trainable params: 1054635 (4.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.src.layers import Embedding, GRU, Dense\n",
    "from keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=n_gram_length-1))\n",
    "\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(GRU(128))\n",
    "\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T13:03:45.261048Z",
     "start_time": "2023-10-11T13:03:45.083745Z"
    }
   },
   "id": "96b229d0057fd8b5"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3245/3245 [==============================] - 184s 55ms/step - loss: 6.8171 - accuracy: 0.0477\n",
      "Epoch 2/50\n",
      "3245/3245 [==============================] - 162s 50ms/step - loss: 6.2582 - accuracy: 0.0673\n",
      "Epoch 3/50\n",
      "3245/3245 [==============================] - 169s 52ms/step - loss: 5.8612 - accuracy: 0.0967\n",
      "Epoch 4/50\n",
      "3245/3245 [==============================] - 163s 50ms/step - loss: 5.5348 - accuracy: 0.1134\n",
      "Epoch 5/50\n",
      "3245/3245 [==============================] - 163s 50ms/step - loss: 5.2643 - accuracy: 0.1237\n",
      "Epoch 6/50\n",
      "3245/3245 [==============================] - 164s 50ms/step - loss: 5.0407 - accuracy: 0.1343\n",
      "Epoch 7/50\n",
      "3245/3245 [==============================] - 165s 51ms/step - loss: 4.8582 - accuracy: 0.1432\n",
      "Epoch 8/50\n",
      "3245/3245 [==============================] - 164s 51ms/step - loss: 4.6981 - accuracy: 0.1515\n",
      "Epoch 9/50\n",
      "3245/3245 [==============================] - 163s 50ms/step - loss: 4.5497 - accuracy: 0.1600\n",
      "Epoch 10/50\n",
      "3245/3245 [==============================] - 163s 50ms/step - loss: 4.4115 - accuracy: 0.1686\n",
      "Epoch 11/50\n",
      "3245/3245 [==============================] - 163s 50ms/step - loss: 4.2831 - accuracy: 0.1770\n",
      "Epoch 12/50\n",
      "3245/3245 [==============================] - 163s 50ms/step - loss: 4.1648 - accuracy: 0.1879\n",
      "Epoch 13/50\n",
      "3245/3245 [==============================] - 163s 50ms/step - loss: 4.0601 - accuracy: 0.1982\n",
      "Epoch 14/50\n",
      "3245/3245 [==============================] - 162s 50ms/step - loss: 3.9631 - accuracy: 0.2091\n",
      "Epoch 15/50\n",
      "3245/3245 [==============================] - 162s 50ms/step - loss: 3.8733 - accuracy: 0.2195\n",
      "Epoch 16/50\n",
      "3245/3245 [==============================] - 162s 50ms/step - loss: 3.7899 - accuracy: 0.2298\n",
      "Epoch 17/50\n",
      "3245/3245 [==============================] - 163s 50ms/step - loss: 3.7106 - accuracy: 0.2410\n",
      "Epoch 18/50\n",
      "3245/3245 [==============================] - 163s 50ms/step - loss: 3.6389 - accuracy: 0.2484\n",
      "Epoch 19/50\n",
      "3245/3245 [==============================] - 163s 50ms/step - loss: 3.5694 - accuracy: 0.2580\n",
      "Epoch 20/50\n",
      "3245/3245 [==============================] - 177s 54ms/step - loss: 3.5027 - accuracy: 0.2672\n",
      "Epoch 21/50\n",
      "3245/3245 [==============================] - 175s 54ms/step - loss: 3.4415 - accuracy: 0.2763\n",
      "Epoch 22/50\n",
      "3245/3245 [==============================] - 173s 53ms/step - loss: 3.3818 - accuracy: 0.2849\n",
      "Epoch 23/50\n",
      "3245/3245 [==============================] - 173s 53ms/step - loss: 3.3263 - accuracy: 0.2936\n",
      "Epoch 24/50\n",
      "3245/3245 [==============================] - 168s 52ms/step - loss: 3.2729 - accuracy: 0.3010\n",
      "Epoch 25/50\n",
      "3245/3245 [==============================] - 171s 53ms/step - loss: 3.2216 - accuracy: 0.3092\n",
      "Epoch 26/50\n",
      "3245/3245 [==============================] - 173s 53ms/step - loss: 3.1752 - accuracy: 0.3156\n",
      "Epoch 27/50\n",
      "3245/3245 [==============================] - 170s 52ms/step - loss: 3.1315 - accuracy: 0.3214\n",
      "Epoch 28/50\n",
      "3245/3245 [==============================] - 169s 52ms/step - loss: 3.0881 - accuracy: 0.3294\n",
      "Epoch 29/50\n",
      "3245/3245 [==============================] - 176s 54ms/step - loss: 3.0464 - accuracy: 0.3363\n",
      "Epoch 30/50\n",
      "3245/3245 [==============================] - 169s 52ms/step - loss: 3.0073 - accuracy: 0.3420\n",
      "Epoch 31/50\n",
      "3245/3245 [==============================] - 185s 57ms/step - loss: 2.9720 - accuracy: 0.3500\n",
      "Epoch 32/50\n",
      "3245/3245 [==============================] - 185s 57ms/step - loss: 2.9385 - accuracy: 0.3546\n",
      "Epoch 33/50\n",
      "3245/3245 [==============================] - 186s 57ms/step - loss: 2.9028 - accuracy: 0.3601\n",
      "Epoch 34/50\n",
      "3245/3245 [==============================] - 182s 56ms/step - loss: 2.8744 - accuracy: 0.3648\n",
      "Epoch 35/50\n",
      "3245/3245 [==============================] - 181s 56ms/step - loss: 2.8414 - accuracy: 0.3705\n",
      "Epoch 36/50\n",
      "3245/3245 [==============================] - 181s 56ms/step - loss: 2.8132 - accuracy: 0.3754\n",
      "Epoch 37/50\n",
      "3245/3245 [==============================] - 174s 54ms/step - loss: 2.7878 - accuracy: 0.3800\n",
      "Epoch 38/50\n",
      "3245/3245 [==============================] - 167s 51ms/step - loss: 2.7597 - accuracy: 0.3835\n",
      "Epoch 39/50\n",
      "3245/3245 [==============================] - 182s 56ms/step - loss: 2.7346 - accuracy: 0.3894\n",
      "Epoch 40/50\n",
      "3245/3245 [==============================] - 186s 57ms/step - loss: 2.7120 - accuracy: 0.3922\n",
      "Epoch 41/50\n",
      "3245/3245 [==============================] - 186s 57ms/step - loss: 2.6895 - accuracy: 0.3973\n",
      "Epoch 42/50\n",
      "3245/3245 [==============================] - 185s 57ms/step - loss: 2.6652 - accuracy: 0.4013\n",
      "Epoch 43/50\n",
      "3245/3245 [==============================] - 182s 56ms/step - loss: 2.6435 - accuracy: 0.4054\n",
      "Epoch 44/50\n",
      "3245/3245 [==============================] - 181s 56ms/step - loss: 2.6275 - accuracy: 0.4083\n",
      "Epoch 45/50\n",
      "3245/3245 [==============================] - 181s 56ms/step - loss: 2.6066 - accuracy: 0.4120\n",
      "Epoch 46/50\n",
      "3245/3245 [==============================] - 180s 55ms/step - loss: 2.5875 - accuracy: 0.4149\n",
      "Epoch 47/50\n",
      "3245/3245 [==============================] - 184s 57ms/step - loss: 2.5706 - accuracy: 0.4185\n",
      "Epoch 48/50\n",
      "3245/3245 [==============================] - 184s 57ms/step - loss: 2.5533 - accuracy: 0.4192\n",
      "Epoch 49/50\n",
      "3245/3245 [==============================] - 204s 63ms/step - loss: 2.5355 - accuracy: 0.4219\n",
      "Epoch 50/50\n",
      "3245/3245 [==============================] - 202s 62ms/step - loss: 2.5208 - accuracy: 0.4261\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x346a1da90>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T15:28:51.467492Z",
     "start_time": "2023-10-11T13:03:48.564984Z"
    }
   },
   "id": "63a05607452391c3"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristian.aars/PycharmProjects/AIAC536-NextWordPrediction/venv/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "\n",
    "model.save(\".models/model_{0}.h5\".format(datetime.datetime.now()).replace(\" \", \"_\"), )\n",
    "\n",
    "with open(\".models/tokenizer_{0}.pickle\".format(datetime.datetime.now()).replace(\" \", \"_\"), 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T15:28:52.788743Z",
     "start_time": "2023-10-11T15:28:50.048907Z"
    }
   },
   "id": "c96235814e505f4b"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "Next predicted words:  Hello there, do you make the rules of\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Hello there, do you make the rules\"\n",
    "next_words = 1\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences(\n",
    "        [token_list],\n",
    "        maxlen=4,\n",
    "        padding='pre'\n",
    "    )\n",
    "    \n",
    "    predictions = model.predict(token_list)\n",
    "    pred_word = tokenizer.index_word[np.argmax(predictions)]\n",
    "    seed_text += \" \" + pred_word\n",
    "\n",
    "print(\"Next predicted words: \", seed_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T15:40:21.427370Z",
     "start_time": "2023-10-11T15:40:21.373849Z"
    }
   },
   "id": "f03606a6d69bdd6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ce1053ccd745c7f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
