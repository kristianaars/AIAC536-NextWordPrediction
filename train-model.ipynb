{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c52e5af3faa1d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f9a2b780d0e3b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446a1e433738b69",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.src.preprocessing.text import Tokenizer\n",
    "from keras.src.utils import pad_sequences\n",
    "from keras.src.utils import to_categorical\n",
    "\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de72e6a28b9537",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Specify dataset version and number of sentences to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d36d7645b7b93c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_version = 'v2'\n",
    "n_sentences = 3000\n",
    "\n",
    "n_gram_length_min = 2\n",
    "n_gram_length_max = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575e2d516f56e9e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_filename = 'blogtext_cleaned{0}.csv'.format(dataset_version)\n",
    "\n",
    "download = not os.path.exists('./' + dataset_filename)\n",
    "\n",
    "if download:\n",
    "    print(\"{0} not found, will attempt to download\".format(dataset_filename))\n",
    "    \n",
    "    if dataset_version == 'v1':\n",
    "        !gdown 16ySojdSN9etEurLs2beGWCJKb6h15bJV\n",
    "    elif dataset_version == 'v2':\n",
    "        !gdown 15El0E261xOjyhapRss9HJ2Fi91Th88jN\n",
    "    else:\n",
    "        raise Exception(\"Unknown dataset version {0}\".format(dataset_version))\n",
    "\n",
    "df = pd.read_csv(dataset_filename, nrows=n_sentences).head(n_sentences)\n",
    "\n",
    "print(\"Loaded {0} rows from {1}\".format(n_sentences, dataset_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356cbf19af494b6e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c68544f4ef987",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_list = df['text'].tolist()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence_list)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a06362ed6fd6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generate n-gram list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339eff71d38e66f7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_gram_list = []\n",
    "\n",
    "for line in sentence_list:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    for n in range(n_gram_length_min, n_gram_length_max):\n",
    "        n_grams = ngrams(token_list, n)\n",
    "        n_gram_list.extend(np.asarray([*n_grams]))\n",
    "\n",
    "# Padding\n",
    "n_gram_list = np.array(pad_sequences(\n",
    "    n_gram_list,\n",
    "    maxlen=n_gram_length_max,\n",
    "    padding='pre'\n",
    "))\n",
    "\n",
    "X = n_gram_list[:, :-1]\n",
    "y = n_gram_list[:, -1]\n",
    "\n",
    "y = to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f795b79949725",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of words: {0}\".format(total_words))\n",
    "print(\"N-gram list length: {0}\".format(len(n_gram_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b470db704e6f3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0712661ec6fe16",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af523f2a52523e7b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.src.layers import Embedding, GRU, Dense, LSTM, RNN\n",
    "from keras import Sequential\n",
    "import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb78bb6cbac0e6b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Specify hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fcf1c61e03e1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "output_dim = 128 # Output dimension of LSTM / GRU / RNN layer\n",
    "activation_func = 'softmax'\n",
    "optimizer = 'adam'\n",
    "\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e781b26b8d3037ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a705618f7f4442",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, embedding_size, input_length=n_gram_length_max-1))\n",
    "\n",
    "model.add(RNN(output_dim))\n",
    "\n",
    "model.add(Dense(total_words, activation=activation_func))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ff28cb998180a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621fabc89620b87",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "checkpoint_path = \"checkpoints/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\" + \"model_checkpoint_{epoch:02d}.h5\"\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(checkpoint_path, save_freq=5000, verbose=1)\n",
    "\n",
    "model.fit(X, y,\n",
    "          epochs=epochs, verbose=1,\n",
    "          callbacks=[tensorboard_callback, checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cef1149d6218b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Persist model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520eaab9a49f316",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save(\"models/model_3.h5\".format(datetime.now()).replace(\" \", \"_\"), save_format='h5')\n",
    "\n",
    "with open(\"models/tokenizer.pickle\".format(datetime.now()).replace(\" \", \"_\"), 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a7d13b3e24d710",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Now we can play with the model!\n",
    "Enter the seed text, and run the cell. The model will predict the most probable next word for your sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead79c07e32feb0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed_text = \"Hello there was a slumber\"\n",
    "next_words = 1\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences(\n",
    "        [token_list],\n",
    "        maxlen=n_gram_length_max - 1,\n",
    "        padding='pre'\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(token_list)\n",
    "    pred_word = tokenizer.index_word[np.argmax(predictions)]\n",
    "    seed_text += \" \" + pred_word\n",
    "\n",
    "print(\"Next predicted words: \", seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
