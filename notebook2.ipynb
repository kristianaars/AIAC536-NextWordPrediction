{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Environment"
   ],
   "metadata": {
    "id": "RIG4mmKB0ZC7"
   },
   "id": "RIG4mmKB0ZC7"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kristian.aars/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/kristian.aars/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kristian.aars/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kristian.aars/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f95d79b0e4965d34",
    "outputId": "11f72bc2-2072-48e0-c6f1-6a2fae9c064d",
    "ExecuteTime": {
     "end_time": "2023-11-29T09:10:35.119636Z",
     "start_time": "2023-11-29T09:10:35.103755Z"
    }
   },
   "id": "f95d79b0e4965d34"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install autocorrect tensorflow numpy keras regex pyyaml h5py contractions pandarallel"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sO636A_4vaWZ",
    "outputId": "c8352f0a-238d-420c-8af4-8e3f59cc90aa",
    "ExecuteTime": {
     "end_time": "2023-11-29T09:10:40.818861Z",
     "start_time": "2023-11-29T09:10:36.155302Z"
    }
   },
   "id": "sO636A_4vaWZ",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in ./venv/lib/python3.11/site-packages (2.6.1)\r\n",
      "Requirement already satisfied: tensorflow in ./venv/lib/python3.11/site-packages (2.14.0)\r\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (1.26.0)\r\n",
      "Requirement already satisfied: keras in ./venv/lib/python3.11/site-packages (2.14.0)\r\n",
      "Requirement already satisfied: regex in ./venv/lib/python3.11/site-packages (2023.10.3)\r\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.11/site-packages (6.0.1)\r\n",
      "Requirement already satisfied: h5py in ./venv/lib/python3.11/site-packages (3.9.0)\r\n",
      "Requirement already satisfied: contractions in ./venv/lib/python3.11/site-packages (0.1.73)\r\n",
      "Requirement already satisfied: pandarallel in ./venv/lib/python3.11/site-packages (1.6.5)\r\n",
      "Requirement already satisfied: tensorflow-macos==2.14.0 in ./venv/lib/python3.11/site-packages (from tensorflow) (2.14.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.0.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.5.26)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.5.4)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (16.0.6)\r\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.24.4)\r\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (65.5.1)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.8.0)\r\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.34.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.59.0)\r\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.1)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in ./venv/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\r\n",
      "Requirement already satisfied: textsearch>=0.0.21 in ./venv/lib/python3.11/site-packages (from contractions) (0.0.24)\r\n",
      "Requirement already satisfied: dill>=0.3.1 in ./venv/lib/python3.11/site-packages (from pandarallel) (0.3.7)\r\n",
      "Requirement already satisfied: pandas>=1 in ./venv/lib/python3.11/site-packages (from pandarallel) (2.1.1)\r\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.11/site-packages (from pandarallel) (5.9.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas>=1->pandarallel) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas>=1->pandarallel) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.11/site-packages (from pandas>=1->pandarallel) (2023.3)\r\n",
      "Requirement already satisfied: anyascii in ./venv/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\r\n",
      "Requirement already satisfied: pyahocorasick in ./venv/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.0->tensorflow) (0.38.4)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./venv/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.23.2)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in ./venv/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.5)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.31.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.7.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.0.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venv/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (5.3.1)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.3.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./venv/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.0.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.1.3)\r\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.5.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.2.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Download blogtext.csv (700 000 blog posts)\n",
    "!gdown 1PJbVYUmRr0_HTwGNtplnu8lG-UCDoXZJ\n",
    "\n",
    "# Download blogtext_cleaned.csv (10 000 blog posts, 43 500 sentences)\n",
    "!gdown 1qI-TZrQ_D0S0g7O3l_qKKA7f4l70jlmf"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7DRnSbjuAWz",
    "outputId": "7741e918-67df-4f9a-a205-3f1bfe53b3b8"
   },
   "id": "t7DRnSbjuAWz",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "id": "ox3MrBPE0eLB"
   },
   "id": "ox3MrBPE0eLB"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "254b9bd8-ff76-4617-fcd3-a28d8334e1e3",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:24:59.082592Z",
     "start_time": "2023-11-29T11:24:52.399805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.74 s, sys: 608 ms, total: 6.35 s\n",
      "Wall time: 6.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Import dataset from CSV\n",
    "\n",
    "df = pd.read_csv('blogtext.csv').head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": "        id gender  age              topic      sign          date  \\\n0  2059027   male   15            Student       Leo   14,May,2004   \n1  2059027   male   15            Student       Leo   13,May,2004   \n2  2059027   male   15            Student       Leo   12,May,2004   \n3  2059027   male   15            Student       Leo   12,May,2004   \n4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n5  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n6  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n7  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n8  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n9  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n\n                                                text  \n0             Info has been found (+/- 100 pages,...  \n1             These are the team members:   Drewe...  \n2             In het kader van kernfusie op aarde...  \n3                   testing!!!  testing!!!            \n4               Thanks to Yahoo!'s Toolbar I can ...  \n5               I had an interesting conversation...  \n6               Somehow Coca-Cola has a way of su...  \n7               If anything, Korea is a country o...  \n8               Take a read of this news article ...  \n9               I surf the English news sites a l...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>topic</th>\n      <th>sign</th>\n      <th>date</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>14,May,2004</td>\n      <td>Info has been found (+/- 100 pages,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>13,May,2004</td>\n      <td>These are the team members:   Drewe...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>12,May,2004</td>\n      <td>In het kader van kernfusie op aarde...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>12,May,2004</td>\n      <td>testing!!!  testing!!!</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>11,June,2004</td>\n      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>I had an interesting conversation...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>Somehow Coca-Cola has a way of su...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>If anything, Korea is a country o...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>10,June,2004</td>\n      <td>Take a read of this news article ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3581210</td>\n      <td>male</td>\n      <td>33</td>\n      <td>InvestmentBanking</td>\n      <td>Aquarius</td>\n      <td>09,June,2004</td>\n      <td>I surf the English news sites a l...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(10)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "377b4d2ee44e3e05",
    "outputId": "f118cebf-da94-4f0c-af2d-8bcf3cffe7e1",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:24:59.086406Z",
     "start_time": "2023-11-29T11:24:59.081871Z"
    }
   },
   "id": "377b4d2ee44e3e05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenize sentences"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d565cedce4c8d393"
   },
   "id": "d565cedce4c8d393"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 412 ms, sys: 7.76 ms, total: 419 ms\n",
      "Wall time: 423 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "(5000, 7)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.text = df.text.transform(lambda t: nltk.sent_tokenize(t))\n",
    "df.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6450521ad0458ab3",
    "outputId": "c11308b2-4248-446b-91d5-6bb28835cbd0",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:24:59.511857Z",
     "start_time": "2023-11-29T11:24:59.135193Z"
    }
   },
   "id": "6450521ad0458ab3"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50918, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(50918, 7)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.explode('text')\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "mask = df['text'].apply(lambda x: isinstance(x, str))\n",
    "df = df[mask]\n",
    "\n",
    "df.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35e293b7db4fb277",
    "outputId": "5489d459-c898-4994-9a3a-33172a5a57a4",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:25:07.838326Z",
     "start_time": "2023-11-29T11:25:07.795732Z"
    }
   },
   "id": "35e293b7db4fb277"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "#df.text.to_csv(\"blogtext-sentence_tokenized.csv\")"
   ],
   "metadata": {
    "id": "54052f0d1e6589d3",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:25:08.451936Z",
     "start_time": "2023-11-29T11:25:08.439739Z"
    }
   },
   "id": "54052f0d1e6589d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepearing data for training"
   ],
   "metadata": {
    "collapsed": false,
    "id": "1aa0d2bb6fe1f060"
   },
   "id": "1aa0d2bb6fe1f060"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "from keras.src.preprocessing.text import Tokenizer\n",
    "from keras.src.utils import pad_sequences, to_categorical\n",
    "import re\n",
    "import random\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import words as en_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "from autocorrect import Speller\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "id": "a8accc7b14d2daeb",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:25:09.978547Z",
     "start_time": "2023-11-29T11:25:09.340383Z"
    }
   },
   "id": "a8accc7b14d2daeb"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def autocorrect_corpus(corpus):\n",
    "    speller = Speller(lang='en')\n",
    "    return corpus.transform(lambda s: speller(s['text']))\n",
    "\n",
    "def has_non_lexi_word(sentence):\n",
    "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "    lemmatized_doc = spacy_nlp(sentence)\n",
    "    \n",
    "    english_words = en_words.words()\n",
    "    \n",
    "    for token in lemmatized_doc:\n",
    "        word = token.lemma_.lower()\n",
    "        \n",
    "        if word not in english_words:                               \n",
    "            #print(\"Found non-english word {0}\".format(word))\n",
    "            return True\n",
    "\n",
    "def contains_number(sentence):\n",
    "    return any(char.isdigit() for char in sentence)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    # Tokenize the input text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Get the list of English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words from the list of words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "def word_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "def clean_corpus(corpus_df, rm_non_lexi_word_sentence=True, rm_contains_num=True, min_word_count=2, remove_stopwords=True, prob_remove_stopword=0.1):\n",
    "    \n",
    "    def remove_sentences_condition(row):\n",
    "        if word_count(row['text']) < min_word_count: return False\n",
    "        elif rm_contains_num and contains_number(row['text']): return False\n",
    "        elif rm_non_lexi_word_sentence and has_non_lexi_word(row['text']): return False\n",
    "        else: return True\n",
    "    \n",
    "    # Function to normalize and clean the text\n",
    "    def clean_text(text):\n",
    "        # Convert to lowercase for better normalization\n",
    "        text = text.lower()\n",
    "    \n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "        # Tokenize the words\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # Remove stop words\n",
    "        if remove_stopwords:\n",
    "            if random.random() < prob_remove_stopword:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                words = [word for word in words if word not in stop_words]\n",
    "                \n",
    "        # Join the cleaned words back into a sentence\n",
    "        cleaned_text = ' '.join(words)\n",
    "    \n",
    "        return cleaned_text\n",
    "    \n",
    "    pre_rem_size = corpus_df.shape[0]\n",
    "    pandarallel.initialize()\n",
    "    corpus_df['text'] = corpus_df['text'].apply(clean_text)\n",
    "    corpus_df = corpus_df[corpus_df.parallel_apply(remove_sentences_condition, axis=1)]\n",
    "    sen_removed = pre_rem_size - corpus_df.shape[0]\n",
    "    print(\"Removed {0} sentences because they contained email, phone, or url(s)\".format(sen_removed))\n",
    "\n",
    "    # Run autocorrect to fix text-typos\n",
    "    # autocorrect_corpus(corpus_df)\n",
    "\n",
    "    # En løsning er å fjerne alle ord som ikke eksisterer i det engelske vokabularet.\n",
    "\n",
    "    return corpus_df\n"
   ],
   "metadata": {
    "id": "cb6ef90035ef7065",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:25:10.476885Z",
     "start_time": "2023-11-29T11:25:10.461993Z"
    }
   },
   "id": "cb6ef90035ef7065"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "Removed 29555 sentences because they contained email, phone, or url(s)\n"
     ]
    }
   ],
   "source": [
    "df = clean_corpus(df, rm_sentence_phone=False, rm_sentence_url=False, rm_sentence_email=False, remove_stopwords=False, rm_non_lexi_word_sentence=True, prob_remove_stopword=0.10)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7284a44ceb21d9ee",
    "outputId": "317a6ce0-783b-4679-a4b1-35452208afdb",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:32:09.097425Z",
     "start_time": "2023-11-29T11:25:16.691731Z"
    }
   },
   "id": "7284a44ceb21d9ee"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "           id  gender  age    topic     sign            date  \\\n2     2059027    male   15  Student      Leo     12,May,2004   \n2     2059027    male   15  Student      Leo     12,May,2004   \n2     2059027    male   15  Student      Leo     12,May,2004   \n2     2059027    male   15  Student      Leo     12,May,2004   \n2     2059027    male   15  Student      Leo     12,May,2004   \n...       ...     ...  ...      ...      ...             ...   \n4995  1103575  female   17   indUnk  Scorpio  27,August,2003   \n4996  1103575  female   17   indUnk  Scorpio  27,August,2003   \n4996  1103575  female   17   indUnk  Scorpio  27,August,2003   \n4999  1103575  female   17   indUnk  Scorpio  26,August,2003   \n4999  1103575  female   17   indUnk  Scorpio  26,August,2003   \n\n                                                   text  \n2     seemed to be a transcript of a seven days article  \n2                        poorly formatted and corrupted  \n2     i have added the text between examine under a ...  \n2         if anyone has the full text please distribute  \n2     i am not responsible for the accuracy of this ...  \n...                                                 ...  \n4995  and everybody just sort of looked at each othe...  \n4996               tomorrow i think i will be depressed  \n4996                                 i think ill go now  \n4999                      when i got home i did laundry  \n4999                                     i like daisies  \n\n[21363 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>topic</th>\n      <th>sign</th>\n      <th>date</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>12,May,2004</td>\n      <td>seemed to be a transcript of a seven days article</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>12,May,2004</td>\n      <td>poorly formatted and corrupted</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>12,May,2004</td>\n      <td>i have added the text between examine under a ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>12,May,2004</td>\n      <td>if anyone has the full text please distribute</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2059027</td>\n      <td>male</td>\n      <td>15</td>\n      <td>Student</td>\n      <td>Leo</td>\n      <td>12,May,2004</td>\n      <td>i am not responsible for the accuracy of this ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>1103575</td>\n      <td>female</td>\n      <td>17</td>\n      <td>indUnk</td>\n      <td>Scorpio</td>\n      <td>27,August,2003</td>\n      <td>and everybody just sort of looked at each othe...</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>1103575</td>\n      <td>female</td>\n      <td>17</td>\n      <td>indUnk</td>\n      <td>Scorpio</td>\n      <td>27,August,2003</td>\n      <td>tomorrow i think i will be depressed</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>1103575</td>\n      <td>female</td>\n      <td>17</td>\n      <td>indUnk</td>\n      <td>Scorpio</td>\n      <td>27,August,2003</td>\n      <td>i think ill go now</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>1103575</td>\n      <td>female</td>\n      <td>17</td>\n      <td>indUnk</td>\n      <td>Scorpio</td>\n      <td>26,August,2003</td>\n      <td>when i got home i did laundry</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>1103575</td>\n      <td>female</td>\n      <td>17</td>\n      <td>indUnk</td>\n      <td>Scorpio</td>\n      <td>26,August,2003</td>\n      <td>i like daisies</td>\n    </tr>\n  </tbody>\n</table>\n<p>21363 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "\n",
    "# Remove sentences with less than two words\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T11:38:24.816450Z",
     "start_time": "2023-11-29T11:38:24.809552Z"
    }
   },
   "id": "5ed172ed9c897eb5"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "df.to_csv(\"blogtext_cleaned.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T11:38:25.502258Z",
     "start_time": "2023-11-29T11:38:25.449579Z"
    }
   },
   "id": "2daf068cf13ab092"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(10000,)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('blogtext_cleanedv2.csv')\n",
    "\n",
    "#df['text'] = df['text'].apply(remove_stop_words)\n",
    "\n",
    "df = df['text'].head(10000)\n",
    "\n",
    "df.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6df1f532b86fec3",
    "outputId": "b02b5230-9ecd-4dc8-d164-d1f031a2edbd",
    "ExecuteTime": {
     "end_time": "2023-11-30T06:08:25.496960Z",
     "start_time": "2023-11-30T06:08:25.446206Z"
    }
   },
   "id": "b6df1f532b86fec3"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "0       seemed to be a transcript of a seven days article\n1                          poorly formatted and corrupted\n2       i have added the text between examine under a ...\n3           if anyone has the full text please distribute\n4       i am not responsible for the accuracy of this ...\n                              ...                        \n9995    and this is not to say that one way is better ...\n9996    we make lots more money here because of our pr...\n9997        its part of what makes us a capitalist nation\n9998    but its also a big part of why were a fat nati...\n9999    i bristle when it is suggested that in the who...\nName: text, Length: 10000, dtype: object"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "770b33009bff8548",
    "outputId": "283cdf04-e1a8-41e4-9361-e912c5cbfcb9",
    "ExecuteTime": {
     "end_time": "2023-11-30T06:08:26.343888Z",
     "start_time": "2023-11-30T06:08:26.341168Z"
    }
   },
   "id": "770b33009bff8548"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T06:08:27.483127Z",
     "start_time": "2023-11-30T06:08:27.460730Z"
    }
   },
   "id": "b4f8d144afe4ebf9"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m sentence_list \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m----> 3\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mTokenizer\u001B[49m()\n\u001B[1;32m      4\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mfit_on_texts(sentence_list)\n\u001B[1;32m      5\u001B[0m total_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mword_index) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "sentence_list = df.tolist()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence_list)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ],
   "metadata": {
    "id": "52eab5627e64f910",
    "ExecuteTime": {
     "end_time": "2023-11-30T06:08:27.931640Z",
     "start_time": "2023-11-30T06:08:27.917402Z"
    }
   },
   "id": "52eab5627e64f910"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "c_dict = {}\n",
    "\n",
    "for s in df:\n",
    "    print(s)\n",
    "    wrds = s.split()\n",
    "    for w in wrds:\n",
    "        if w in c_dict.keys():\n",
    "            c_dict[w] += 1\n",
    "        else:\n",
    "            c_dict[w] = 1\n",
    "            \n",
    "c_dict = dict(sorted(c_dict.items(), key=lambda item: item[1]))\n",
    "\n",
    "# Extract words and counts\n",
    "words = list(c_dict.keys())\n",
    "counts = list(c_dict.values())\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, counts, color='blue')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Word Count Distribution')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.show()\n",
    "\n",
    "c_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-29T11:57:48.116056Z"
    }
   },
   "id": "993fdf65cc43e61c"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "8802"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11d1f4229e029de6",
    "outputId": "450e358c-d89d-4af8-ffd1-8f41021eafbb",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:57:50.566487Z",
     "start_time": "2023-11-29T11:57:50.562022Z"
    }
   },
   "id": "11d1f4229e029de6"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "{'the': 1,\n 'i': 2,\n 'to': 3,\n 'and': 4,\n 'a': 5,\n 'of': 6,\n 'that': 7,\n 'it': 8,\n 'in': 9,\n 'you': 10,\n 'is': 11,\n 'my': 12,\n 'was': 13,\n 'for': 14,\n 'so': 15,\n 'this': 16,\n 'me': 17,\n 'have': 18,\n 'on': 19,\n 'but': 20,\n 'not': 21,\n 'be': 22,\n 'he': 23,\n 'with': 24,\n 'im': 25,\n 'we': 26,\n 'all': 27,\n 'what': 28,\n 'are': 29,\n 'its': 30,\n 'just': 31,\n 'they': 32,\n 'like': 33,\n 'one': 34,\n 'at': 35,\n 'as': 36,\n 'do': 37,\n 'can': 38,\n 'there': 39,\n 'or': 40,\n 'up': 41,\n 'about': 42,\n 'will': 43,\n 'when': 44,\n 'out': 45,\n 'no': 46,\n 'if': 47,\n 'had': 48,\n 'get': 49,\n 'now': 50,\n 'think': 51,\n 'really': 52,\n 'time': 53,\n 'your': 54,\n 'more': 55,\n 'his': 56,\n 'from': 57,\n 'know': 58,\n 'good': 59,\n 'she': 60,\n 'well': 61,\n 'go': 62,\n 'were': 63,\n 'would': 64,\n 'her': 65,\n 'how': 66,\n 'then': 67,\n 'people': 68,\n 'am': 69,\n 'an': 70,\n 'them': 71,\n 'got': 72,\n 'some': 73,\n 'day': 74,\n 'much': 75,\n 'too': 76,\n 'love': 77,\n 'him': 78,\n 'see': 79,\n 'been': 80,\n 'going': 81,\n 'could': 82,\n 'by': 83,\n 'back': 84,\n 'has': 85,\n 'who': 86,\n 'thats': 87,\n 'never': 88,\n 'their': 89,\n 'right': 90,\n 'way': 91,\n 'only': 92,\n 'today': 93,\n 'want': 94,\n 'something': 95,\n 'ive': 96,\n 'still': 97,\n 'life': 98,\n 'other': 99,\n 'into': 100,\n 'things': 101,\n 'thing': 102,\n 'than': 103,\n 'here': 104,\n 'need': 105,\n 'because': 106,\n 'make': 107,\n 'thought': 108,\n 'over': 109,\n 'again': 110,\n 'very': 111,\n 'little': 112,\n 'did': 113,\n 'say': 114,\n 'our': 115,\n 'last': 116,\n 'where': 117,\n 'friends': 118,\n 'down': 119,\n 'even': 120,\n 'off': 121,\n 'ill': 122,\n 'first': 123,\n 'work': 124,\n 'after': 125,\n 'new': 126,\n 'why': 127,\n 'went': 128,\n 'around': 129,\n 'always': 130,\n 'said': 131,\n 'should': 132,\n 'fun': 133,\n 'oh': 134,\n 'before': 135,\n 'take': 136,\n 'guess': 137,\n 'which': 138,\n 'few': 139,\n 'pretty': 140,\n 'being': 141,\n 'maybe': 142,\n 'feel': 143,\n 'world': 144,\n 'us': 145,\n 'many': 146,\n 'nothing': 147,\n 'look': 148,\n 'though': 149,\n 'ever': 150,\n 'come': 151,\n 'most': 152,\n 'every': 153,\n 'youre': 154,\n 'through': 155,\n 'any': 156,\n 'home': 157,\n 'away': 158,\n 'two': 159,\n 'another': 160,\n 'night': 161,\n 'while': 162,\n 'also': 163,\n 'mean': 164,\n 'place': 165,\n 'great': 166,\n 'actually': 167,\n 'made': 168,\n 'hate': 169,\n 'these': 170,\n 'happy': 171,\n 'find': 172,\n 'better': 173,\n 'next': 174,\n 'sure': 175,\n 'does': 176,\n 'those': 177,\n 'hope': 178,\n 'yeah': 179,\n 'long': 180,\n 'someone': 181,\n 'days': 182,\n 'everything': 183,\n 'school': 184,\n 'same': 185,\n 'person': 186,\n 'everyone': 187,\n 'week': 188,\n 'let': 189,\n 'tell': 190,\n 'myself': 191,\n 'may': 192,\n 'read': 193,\n 'getting': 194,\n 'best': 195,\n 'yes': 196,\n 'doing': 197,\n 'man': 198,\n 'least': 199,\n 'year': 200,\n 'left': 201,\n 'job': 202,\n 'id': 203,\n 'bad': 204,\n 'came': 205,\n 'house': 206,\n 'big': 207,\n 'quite': 208,\n 'morning': 209,\n 'lot': 210,\n 'thinking': 211,\n 'wanted': 212,\n 'own': 213,\n 'give': 214,\n 'nice': 215,\n 'anyone': 216,\n 'makes': 217,\n 'hes': 218,\n 'anything': 219,\n 'na': 220,\n 'use': 221,\n 'put': 222,\n 'cause': 223,\n 'theres': 224,\n 'enough': 225,\n 'whole': 226,\n 'yet': 227,\n 'knew': 228,\n 'anyway': 229,\n 'seems': 230,\n 'cool': 231,\n 'bit': 232,\n 'else': 233,\n 'wish': 234,\n 'keep': 235,\n 'must': 236,\n 'sometimes': 237,\n 'such': 238,\n 'hard': 239,\n 'friend': 240,\n 'having': 241,\n 'write': 242,\n 'each': 243,\n 'probably': 244,\n 'god': 245,\n 'together': 246,\n 'point': 247,\n 'done': 248,\n 'head': 249,\n 'since': 250,\n 'show': 251,\n 'stuff': 252,\n 'help': 253,\n 'guys': 254,\n 'funny': 255,\n 'might': 256,\n 'almost': 257,\n 'interesting': 258,\n 'old': 259,\n 'different': 260,\n 'car': 261,\n 'years': 262,\n 'try': 263,\n 'real': 264,\n 'started': 265,\n 'found': 266,\n 'call': 267,\n 'change': 268,\n 'later': 269,\n 'girl': 270,\n 'kind': 271,\n 'past': 272,\n 'start': 273,\n 'family': 274,\n 'game': 275,\n 'told': 276,\n 'trying': 277,\n 'times': 278,\n 'side': 279,\n 'already': 280,\n 'mind': 281,\n 'once': 282,\n 'however': 283,\n 'without': 284,\n 'talk': 285,\n 'fact': 286,\n 'hell': 287,\n 'talking': 288,\n 'end': 289,\n 'far': 290,\n 'face': 291,\n 'play': 292,\n 'eyes': 293,\n 'u': 294,\n 'asked': 295,\n 'sleep': 296,\n 'looking': 297,\n 'saw': 298,\n 'yesterday': 299,\n 'live': 300,\n 'remember': 301,\n 'money': 302,\n 'took': 303,\n 'wrong': 304,\n 'reading': 305,\n 'tomorrow': 306,\n 'part': 307,\n 'story': 308,\n 'room': 309,\n 'course': 310,\n 'summer': 311,\n 'free': 312,\n 'idea': 313,\n 'phone': 314,\n 'felt': 315,\n 'happened': 316,\n 'making': 317,\n 'gon': 318,\n 'book': 319,\n 'hand': 320,\n 'often': 321,\n 'feeling': 322,\n 'post': 323,\n 'others': 324,\n 'soon': 325,\n 'music': 326,\n 'words': 327,\n 'wonderful': 328,\n 'used': 329,\n 'stop': 330,\n 'theyre': 331,\n 'sad': 332,\n 'movie': 333,\n 'leave': 334,\n 'late': 335,\n 'beach': 336,\n 'sorry': 337,\n 'guy': 338,\n 'city': 339,\n 'party': 340,\n 'damn': 341,\n 'both': 342,\n 'tonight': 343,\n 'believe': 344,\n 'finally': 345,\n 'beautiful': 346,\n 'saying': 347,\n 'taking': 348,\n 'name': 349,\n 'lost': 350,\n 'whatever': 351,\n 'run': 352,\n 'hair': 353,\n 'sick': 354,\n 'song': 355,\n 'awesome': 356,\n 'decided': 357,\n 'three': 358,\n 'yourself': 359,\n 'working': 360,\n 'early': 361,\n 'couple': 362,\n 'class': 363,\n 'question': 364,\n 'seen': 365,\n 'happen': 366,\n 'especially': 367,\n 'race': 368,\n 'ask': 369,\n 'boy': 370,\n 'under': 371,\n 'sense': 372,\n 'wonder': 373,\n 'ones': 374,\n 'men': 375,\n 'alone': 376,\n 'experience': 377,\n 'heart': 378,\n 'true': 379,\n 'move': 380,\n 'along': 381,\n 'weekend': 382,\n 'gone': 383,\n 'coming': 384,\n 'possible': 385,\n 'goes': 386,\n 'short': 387,\n 'reason': 388,\n 'ya': 389,\n 'playing': 390,\n 'walk': 391,\n 'set': 392,\n 'college': 393,\n 'happens': 394,\n 'living': 395,\n 'high': 396,\n 'hot': 397,\n 'fear': 398,\n 'light': 399,\n 'ta': 400,\n 'small': 401,\n 'half': 402,\n 'heard': 403,\n 'outside': 404,\n 'mother': 405,\n 'comes': 406,\n 'bed': 407,\n 'parents': 408,\n 'less': 409,\n 'turn': 410,\n 'food': 411,\n 'hey': 412,\n 'boring': 413,\n 'stupid': 414,\n 'tired': 415,\n 'lets': 416,\n 'amazing': 417,\n 'gets': 418,\n 'hit': 419,\n 'shes': 420,\n 'between': 421,\n 'pain': 422,\n 'problem': 423,\n 'matter': 424,\n 'means': 425,\n 'moment': 426,\n 'death': 427,\n 'favorite': 428,\n 'learn': 429,\n 'miss': 430,\n 'sit': 431,\n 'looked': 432,\n 'cold': 433,\n 'whats': 434,\n 'tried': 435,\n 'dad': 436,\n 'rather': 437,\n 'team': 438,\n 'girls': 439,\n 'needed': 440,\n 'hear': 441,\n 'huge': 442,\n 'listen': 443,\n 'water': 444,\n 'sound': 445,\n 'teams': 446,\n 'full': 447,\n 'please': 448,\n 'called': 449,\n 'air': 450,\n 'eat': 451,\n 'inside': 452,\n 'knows': 453,\n 'figure': 454,\n 'stay': 455,\n 'second': 456,\n 'open': 457,\n 'close': 458,\n 'watching': 459,\n 'until': 460,\n 'thank': 461,\n 'fell': 462,\n 'door': 463,\n 'turned': 464,\n 'check': 465,\n 'easy': 466,\n 'wake': 467,\n 'bus': 468,\n 'running': 469,\n 'important': 470,\n 'news': 471,\n 'waiting': 472,\n 'either': 473,\n 'o': 474,\n 'care': 475,\n 'spent': 476,\n 'during': 477,\n 'exactly': 478,\n 'word': 479,\n 'employer': 480,\n 'birthday': 481,\n 'longer': 482,\n 'although': 483,\n 'above': 484,\n 'walking': 485,\n 'perfect': 486,\n 'die': 487,\n 'wait': 488,\n 'says': 489,\n 'certain': 490,\n 'power': 491,\n 'usually': 492,\n 'store': 493,\n 'red': 494,\n 'answer': 495,\n 'pay': 496,\n 'ago': 497,\n 'weird': 498,\n 'mine': 499,\n 'front': 500,\n 'ready': 501,\n 'fall': 502,\n 'pool': 503,\n 'sweet': 504,\n 'quickly': 505,\n 'sounds': 506,\n 'rest': 507,\n 'order': 508,\n 'completely': 509,\n 'save': 510,\n 'pictures': 511,\n 'special': 512,\n 'future': 513,\n 'movies': 514,\n 'young': 515,\n 'weeks': 516,\n 'against': 517,\n 'games': 518,\n 'hands': 519,\n 'body': 520,\n 'able': 521,\n 'gave': 522,\n 'sort': 523,\n 'mood': 524,\n 'band': 525,\n 'worth': 526,\n 'sister': 527,\n 'wants': 528,\n 'coffee': 529,\n 'buy': 530,\n 'forget': 531,\n 'upon': 532,\n 'case': 533,\n 'type': 534,\n 'instead': 535,\n 'needs': 536,\n 'lives': 537,\n 'ended': 538,\n 'picture': 539,\n 'driving': 540,\n 'looks': 541,\n 'fine': 542,\n 'break': 543,\n 'ran': 544,\n 'window': 545,\n 'cut': 546,\n 'hours': 547,\n 'luck': 548,\n 'letter': 549,\n 'glad': 550,\n 'talked': 551,\n 'group': 552,\n 'seemed': 553,\n 'fast': 554,\n 'sun': 555,\n 'country': 556,\n 'road': 557,\n 'wife': 558,\n 'trip': 559,\n 'office': 560,\n 'within': 561,\n 'loved': 562,\n 'liked': 563,\n 'across': 564,\n 'bridge': 565,\n 'black': 566,\n 'become': 567,\n 'woman': 568,\n 'children': 569,\n 'green': 570,\n 'lots': 571,\n 'company': 572,\n 'thoughts': 573,\n 'using': 574,\n 'sitting': 575,\n 'coz': 576,\n 'hold': 577,\n 'questions': 578,\n 'self': 579,\n 'five': 580,\n 'seem': 581,\n 'state': 582,\n 'problems': 583,\n 'dreams': 584,\n 'poor': 585,\n 'eating': 586,\n 'apple': 587,\n 'enjoy': 588,\n 'none': 589,\n 'station': 590,\n 'sucks': 591,\n 'simple': 592,\n 'dark': 593,\n 'cover': 594,\n 'drive': 595,\n 'career': 596,\n 'strange': 597,\n 'perhaps': 598,\n 'imagine': 599,\n 'street': 600,\n 'dream': 601,\n 'chance': 602,\n 'watch': 603,\n 'age': 604,\n 'add': 605,\n 'youve': 606,\n 'horrible': 607,\n 'pass': 608,\n 'mothers': 609,\n 'walked': 610,\n 'seriously': 611,\n 'mention': 612,\n 'wan': 613,\n 'telling': 614,\n 'baby': 615,\n 'space': 616,\n 'catch': 617,\n 'sign': 618,\n 'takes': 619,\n 'somehow': 620,\n 'hole': 621,\n 'ground': 622,\n 'cousin': 623,\n 'top': 624,\n 'create': 625,\n 'ass': 626,\n 'trouble': 627,\n 'wondering': 628,\n 'thanks': 629,\n 'key': 630,\n 'stand': 631,\n 'women': 632,\n 'control': 633,\n 'met': 634,\n 'deal': 635,\n 'apartment': 636,\n 'gay': 637,\n 'leg': 638,\n 'writing': 639,\n 'deep': 640,\n 'season': 641,\n 'rain': 642,\n 'kept': 643,\n 'fair': 644,\n 'hero': 645,\n 'wind': 646,\n 'comments': 647,\n 'shall': 648,\n 'played': 649,\n 'hopefully': 650,\n 'mostly': 651,\n 'corner': 652,\n 'books': 653,\n 'hour': 654,\n 'sat': 655,\n 'harpers': 656,\n 'uranium': 657,\n 'glass': 658,\n 'easier': 659,\n 'known': 660,\n 'absolutely': 661,\n 'earth': 662,\n 'human': 663,\n 'starts': 664,\n 'area': 665,\n 'level': 666,\n 'meet': 667,\n 'understand': 668,\n 'smile': 669,\n 'busy': 670,\n 'married': 671,\n 'beyond': 672,\n 'nature': 673,\n 'somewhere': 674,\n 'line': 675,\n 'list': 676,\n 'eye': 677,\n 'spend': 678,\n 'realize': 679,\n 'mouth': 680,\n 'computer': 681,\n 'month': 682,\n 'skills': 683,\n 'bob': 684,\n 'form': 685,\n 'note': 686,\n 'blood': 687,\n 'site': 688,\n 'chip': 689,\n 'expected': 690,\n 'basically': 691,\n 'changed': 692,\n 'starting': 693,\n 'taken': 694,\n 'works': 695,\n 'view': 696,\n 'forever': 697,\n 'knowing': 698,\n 'secret': 699,\n 'middle': 700,\n 'given': 701,\n 'missed': 702,\n 'win': 703,\n 'character': 704,\n 'heres': 705,\n 'current': 706,\n 'calls': 707,\n 'mad': 708,\n 'laughing': 709,\n 'finish': 710,\n 'lady': 711,\n 'portfolio': 712,\n 'clue': 713,\n 'information': 714,\n 'share': 715,\n 'amount': 716,\n 'step': 717,\n 'business': 718,\n 'conversation': 719,\n 'tears': 720,\n 'otherwise': 721,\n 'pattern': 722,\n 'four': 723,\n 'plus': 724,\n 'low': 725,\n 'dear': 726,\n 'listening': 727,\n 'normal': 728,\n 'weight': 729,\n 'worse': 730,\n 'brought': 731,\n 'attention': 732,\n 'thin': 733,\n 'provide': 734,\n 'entire': 735,\n 'strong': 736,\n 'cute': 737,\n 'support': 738,\n 'teacher': 739,\n 'giving': 740,\n 'nobody': 741,\n 'kill': 742,\n 'bought': 743,\n 'definitely': 744,\n 'dead': 745,\n 'realized': 746,\n 'worked': 747,\n 'bag': 748,\n 'camp': 749,\n 'themselves': 750,\n 'floor': 751,\n 'straight': 752,\n 'comment': 753,\n 'dog': 754,\n 'places': 755,\n 'everywhere': 756,\n 'losing': 757,\n 'below': 758,\n 'expect': 759,\n 'watched': 760,\n 'memory': 761,\n 'older': 762,\n 'crazy': 763,\n 'local': 764,\n 'scary': 765,\n 'supposed': 766,\n 'interest': 767,\n 'seeing': 768,\n 'himself': 769,\n 'position': 770,\n 'choice': 771,\n 'stuck': 772,\n 'afraid': 773,\n 'act': 774,\n 'decide': 775,\n 'round': 776,\n 'brothers': 777,\n 'la': 778,\n 'child': 779,\n 'opened': 780,\n 'doubt': 781,\n 'closer': 782,\n 'serious': 783,\n 'companies': 784,\n 'follow': 785,\n 'ice': 786,\n 'voice': 787,\n 'honestly': 788,\n 'quiet': 789,\n 'b': 790,\n 'link': 791,\n 'cousins': 792,\n 'stories': 793,\n 'behind': 794,\n 'posts': 795,\n 'excited': 796,\n 'faith': 797,\n 'weve': 798,\n 'knitting': 799,\n 'somebody': 800,\n 'white': 801,\n 'large': 802,\n 'hello': 803,\n 'truth': 804,\n 'stopped': 805,\n 'alright': 806,\n 'throw': 807,\n 'shopping': 808,\n 'asking': 809,\n 'speaking': 810,\n 'anyways': 811,\n 'confused': 812,\n 'joke': 813,\n 'service': 814,\n 'continue': 815,\n 'dancing': 816,\n 'player': 817,\n 'card': 818,\n 'apparently': 819,\n 'colors': 820,\n 'moments': 821,\n 'teachers': 822,\n 'minute': 823,\n 'weather': 824,\n 'quick': 825,\n 'church': 826,\n 'except': 827,\n 'suck': 828,\n 'blue': 829,\n 'shell': 830,\n 'n': 831,\n 'table': 832,\n 'afternoon': 833,\n 'sudden': 834,\n 'plan': 835,\n 'passed': 836,\n 'issue': 837,\n 'pick': 838,\n 'laugh': 839,\n 'bring': 840,\n 'turns': 841,\n 'ride': 842,\n 'suddenly': 843,\n 'warm': 844,\n 'posting': 845,\n 'oil': 846,\n 'sex': 847,\n 'blah': 848,\n 'daddy': 849,\n 'bush': 850,\n 'track': 851,\n 'employers': 852,\n 'energy': 853,\n 'difficult': 854,\n 'cash': 855,\n 'meant': 856,\n 'totally': 857,\n 'test': 858,\n 'rocks': 859,\n 'finished': 860,\n 'hed': 861,\n 'worst': 862,\n 'explain': 863,\n 'huh': 864,\n 'odd': 865,\n 'beginning': 866,\n 'lately': 867,\n 'number': 868,\n 'pull': 869,\n 'ah': 870,\n 'cards': 871,\n 'leaving': 872,\n 'stayed': 873,\n 'crying': 874,\n 'wall': 875,\n 'praise': 876,\n 'path': 877,\n 'emotion': 878,\n 'anybody': 879,\n 'broke': 880,\n 'fat': 881,\n 'crap': 882,\n 'evil': 883,\n 'indeed': 884,\n 'tea': 885,\n 'fight': 886,\n 'seat': 887,\n 'months': 888,\n 'welcome': 889,\n 'depressed': 890,\n 'spare': 891,\n 'filled': 892,\n 'enjoyable': 893,\n 'glory': 894,\n 'happening': 895,\n 'rock': 896,\n 'folks': 897,\n 'grow': 898,\n 'meaning': 899,\n 'feelings': 900,\n 'standing': 901,\n 'jobs': 902,\n 'ahead': 903,\n 'whos': 904,\n 'brain': 905,\n 'individual': 906,\n 'herself': 907,\n 'autism': 908,\n 'golf': 909,\n 'alison': 910,\n 'missing': 911,\n 'apart': 912,\n 'ways': 913,\n 'begin': 914,\n 'government': 915,\n 'market': 916,\n 'join': 917,\n 'moving': 918,\n 'vacation': 919,\n 'everyday': 920,\n 'whether': 921,\n 'son': 922,\n 'system': 923,\n 'notice': 924,\n 'lazy': 925,\n 'calling': 926,\n 'twice': 927,\n 'literally': 928,\n 'clean': 929,\n 'father': 930,\n 'blow': 931,\n 'lesson': 932,\n 'building': 933,\n 'closed': 934,\n 'record': 935,\n 'police': 936,\n 'ideas': 937,\n 'process': 938,\n 'hurt': 939,\n 'fingers': 940,\n 'becomes': 941,\n 'clear': 942,\n 'everybody': 943,\n 'reality': 944,\n 'lucky': 945,\n 'extremely': 946,\n 'biggest': 947,\n 'truly': 948,\n 'subject': 949,\n 'personality': 950,\n 'smoking': 951,\n 'tiny': 952,\n 'learning': 953,\n 'childhood': 954,\n 'eventually': 955,\n 'lunch': 956,\n 'lose': 957,\n 'bloody': 958,\n 'dinner': 959,\n 'clothes': 960,\n 'obviously': 961,\n 'lines': 962,\n 'breakfast': 963,\n 'itself': 964,\n 'characters': 965,\n 'stairs': 966,\n 'fill': 967,\n 'paid': 968,\n 'price': 969,\n 'sent': 970,\n 'lack': 971,\n 'setting': 972,\n 'exciting': 973,\n 'thinks': 974,\n 'moon': 975,\n 'whom': 976,\n 'figured': 977,\n 'nights': 978,\n 'among': 979,\n 'classes': 980,\n 'ball': 981,\n 'wow': 982,\n 'return': 983,\n 'due': 984,\n 'wedding': 985,\n 'resume': 986,\n 'windows': 987,\n 'album': 988,\n 'loves': 989,\n 'history': 990,\n 'gift': 991,\n 'bitch': 992,\n 'woke': 993,\n 'entry': 994,\n 'war': 995,\n 'push': 996,\n 'likely': 997,\n 'broken': 998,\n 'plastic': 999,\n 'recommend': 1000,\n ...}"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff31f871b9c4f0e9",
    "outputId": "ecbe00fe-f779-4280-ce68-50cb421a6a16",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:57:51.785550Z",
     "start_time": "2023-11-29T11:57:51.776692Z"
    }
   },
   "id": "ff31f871b9c4f0e9"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "n_gram_list = []\n",
    "n_gram_length = 3\n",
    "\n",
    "for line in sentence_list:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    for n in range(2, n_gram_length):\n",
    "        n_grams = ngrams(token_list, n)\n",
    "        n_gram_list.extend(np.asarray([*n_grams]))\n",
    "\n",
    "# Padding\n",
    "n_gram_list = np.array(pad_sequences(\n",
    "    n_gram_list,\n",
    "    maxlen=n_gram_length,\n",
    "    padding='pre'\n",
    "))"
   ],
   "metadata": {
    "id": "a636c919dc7fcba1",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:57:52.776819Z",
     "start_time": "2023-11-29T11:57:52.658726Z"
    }
   },
   "id": "a636c919dc7fcba1"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "X = n_gram_list[:, :-1]\n",
    "y = n_gram_list[:, -1]\n",
    "\n",
    "y = to_categorical(y, num_classes=total_words)"
   ],
   "metadata": {
    "id": "b254b0558e1de286",
    "ExecuteTime": {
     "end_time": "2023-11-29T11:57:56.732868Z",
     "start_time": "2023-11-29T11:57:56.391588Z"
    }
   },
   "id": "b254b0558e1de286"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build and Train Model"
   ],
   "metadata": {
    "id": "Zjf-vBhl0oHA"
   },
   "id": "Zjf-vBhl0oHA"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 2, 128)            1126656   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 256)               394240    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8802)              2262114   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3783010 (14.43 MB)\n",
      "Trainable params: 3783010 (14.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.src.layers import Embedding, GRU, Dense, LSTM\n",
    "from keras import Sequential\n",
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 128, input_length=n_gram_length-1))\n",
    "\n",
    "model.add(LSTM(256))\n",
    "\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96b229d0057fd8b5",
    "outputId": "c9f23548-9050-4580-9f3b-6bef82dc4343",
    "ExecuteTime": {
     "end_time": "2023-11-29T12:14:20.031839Z",
     "start_time": "2023-11-29T12:14:19.848070Z"
    }
   },
   "id": "96b229d0057fd8b5"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3131/3131 [==============================] - 64s 20ms/step - loss: 6.5341 - accuracy: 0.0555\n",
      "Epoch 2/100\n",
      "1866/3131 [================>.............] - ETA: 24s - loss: 5.9126 - accuracy: 0.1015\n",
      "Epoch 2: saving model to checkpoints/20231129-175920/model_checkpoint_02.h5\n",
      "3131/3131 [==============================] - 63s 20ms/step - loss: 5.8808 - accuracy: 0.1048\n",
      "Epoch 3/100\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 5.5661 - accuracy: 0.1238\n",
      "Epoch 4/100\n",
      " 606/3131 [====>.........................] - ETA: 45s - loss: 5.2698 - accuracy: 0.1344\n",
      "Epoch 4: saving model to checkpoints/20231129-175920/model_checkpoint_04.h5\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 5.3395 - accuracy: 0.1383\n",
      "Epoch 5/100\n",
      "2475/3131 [======================>.......] - ETA: 13s - loss: 5.1166 - accuracy: 0.1500\n",
      "Epoch 5: saving model to checkpoints/20231129-175920/model_checkpoint_05.h5\n",
      "3131/3131 [==============================] - 63s 20ms/step - loss: 5.1369 - accuracy: 0.1493\n",
      "Epoch 6/100\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.9550 - accuracy: 0.1590\n",
      "Epoch 7/100\n",
      "1211/3131 [==========>...................] - ETA: 39s - loss: 4.7140 - accuracy: 0.1683\n",
      "Epoch 7: saving model to checkpoints/20231129-175920/model_checkpoint_07.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.7980 - accuracy: 0.1661\n",
      "Epoch 8/100\n",
      "3080/3131 [============================>.] - ETA: 1s - loss: 4.6655 - accuracy: 0.1715\n",
      "Epoch 8: saving model to checkpoints/20231129-175920/model_checkpoint_08.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.6667 - accuracy: 0.1714\n",
      "Epoch 9/100\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.5599 - accuracy: 0.1732\n",
      "Epoch 10/100\n",
      "1818/3131 [================>.............] - ETA: 27s - loss: 4.4202 - accuracy: 0.1787\n",
      "Epoch 10: saving model to checkpoints/20231129-175920/model_checkpoint_10.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.4770 - accuracy: 0.1742\n",
      "Epoch 11/100\n",
      "3131/3131 [==============================] - 63s 20ms/step - loss: 4.4150 - accuracy: 0.1732\n",
      "Epoch 12/100\n",
      " 556/3131 [====>.........................] - ETA: 51s - loss: 4.1645 - accuracy: 0.1881\n",
      "Epoch 12: saving model to checkpoints/20231129-175920/model_checkpoint_12.h5\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.3650 - accuracy: 0.1719\n",
      "Epoch 13/100\n",
      "2425/3131 [======================>.......] - ETA: 14s - loss: 4.2909 - accuracy: 0.1749\n",
      "Epoch 13: saving model to checkpoints/20231129-175920/model_checkpoint_13.h5\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.3270 - accuracy: 0.1723\n",
      "Epoch 14/100\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.2961 - accuracy: 0.1705\n",
      "Epoch 15/100\n",
      "1164/3131 [==========>...................] - ETA: 38s - loss: 4.1595 - accuracy: 0.1788\n",
      "Epoch 15: saving model to checkpoints/20231129-175920/model_checkpoint_15.h5\n",
      "3131/3131 [==============================] - 61s 20ms/step - loss: 4.2688 - accuracy: 0.1716\n",
      "Epoch 16/100\n",
      "3034/3131 [============================>.] - ETA: 1s - loss: 4.2432 - accuracy: 0.1716\n",
      "Epoch 16: saving model to checkpoints/20231129-175920/model_checkpoint_16.h5\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.2465 - accuracy: 0.1714\n",
      "Epoch 17/100\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.2265 - accuracy: 0.1717\n",
      "Epoch 18/100\n",
      "1772/3131 [===============>..............] - ETA: 27s - loss: 4.1340 - accuracy: 0.1770\n",
      "Epoch 18: saving model to checkpoints/20231129-175920/model_checkpoint_18.h5\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.2085 - accuracy: 0.1717\n",
      "Epoch 19/100\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.1943 - accuracy: 0.1711\n",
      "Epoch 20/100\n",
      " 509/3131 [===>..........................] - ETA: 51s - loss: 3.9960 - accuracy: 0.1860\n",
      "Epoch 20: saving model to checkpoints/20231129-175920/model_checkpoint_20.h5\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.1795 - accuracy: 0.1705\n",
      "Epoch 21/100\n",
      "2378/3131 [=====================>........] - ETA: 14s - loss: 4.1315 - accuracy: 0.1730\n",
      "Epoch 21: saving model to checkpoints/20231129-175920/model_checkpoint_21.h5\n",
      "3131/3131 [==============================] - 61s 20ms/step - loss: 4.1674 - accuracy: 0.1710\n",
      "Epoch 22/100\n",
      "3131/3131 [==============================] - 61s 19ms/step - loss: 4.1572 - accuracy: 0.1721\n",
      "Epoch 23/100\n",
      "1117/3131 [=========>....................] - ETA: 40s - loss: 4.0296 - accuracy: 0.1781\n",
      "Epoch 23: saving model to checkpoints/20231129-175920/model_checkpoint_23.h5\n",
      "3131/3131 [==============================] - 63s 20ms/step - loss: 4.1448 - accuracy: 0.1722\n",
      "Epoch 24/100\n",
      "2985/3131 [===========================>..] - ETA: 2s - loss: 4.1297 - accuracy: 0.1717\n",
      "Epoch 24: saving model to checkpoints/20231129-175920/model_checkpoint_24.h5\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.1354 - accuracy: 0.1712\n",
      "Epoch 25/100\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.1273 - accuracy: 0.1721\n",
      "Epoch 26/100\n",
      "1723/3131 [===============>..............] - ETA: 28s - loss: 4.0443 - accuracy: 0.1782\n",
      "Epoch 26: saving model to checkpoints/20231129-175920/model_checkpoint_26.h5\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.1192 - accuracy: 0.1730\n",
      "Epoch 27/100\n",
      "3131/3131 [==============================] - 63s 20ms/step - loss: 4.1109 - accuracy: 0.1731\n",
      "Epoch 28/100\n",
      " 462/3131 [===>..........................] - ETA: 54s - loss: 3.9425 - accuracy: 0.1893\n",
      "Epoch 28: saving model to checkpoints/20231129-175920/model_checkpoint_28.h5\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.1045 - accuracy: 0.1720\n",
      "Epoch 29/100\n",
      "2330/3131 [=====================>........] - ETA: 15s - loss: 4.0570 - accuracy: 0.1744\n",
      "Epoch 29: saving model to checkpoints/20231129-175920/model_checkpoint_29.h5\n",
      "3131/3131 [==============================] - 63s 20ms/step - loss: 4.0965 - accuracy: 0.1716\n",
      "Epoch 30/100\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.0911 - accuracy: 0.1724\n",
      "Epoch 31/100\n",
      "1069/3131 [=========>....................] - ETA: 40s - loss: 3.9874 - accuracy: 0.1796\n",
      "Epoch 31: saving model to checkpoints/20231129-175920/model_checkpoint_31.h5\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.0847 - accuracy: 0.1718\n",
      "Epoch 32/100\n",
      "2936/3131 [===========================>..] - ETA: 3s - loss: 4.0720 - accuracy: 0.1742\n",
      "Epoch 32: saving model to checkpoints/20231129-175920/model_checkpoint_32.h5\n",
      "3131/3131 [==============================] - 63s 20ms/step - loss: 4.0788 - accuracy: 0.1735\n",
      "Epoch 33/100\n",
      "3131/3131 [==============================] - 63s 20ms/step - loss: 4.0728 - accuracy: 0.1729\n",
      "Epoch 34/100\n",
      "1676/3131 [===============>..............] - ETA: 29s - loss: 4.0114 - accuracy: 0.1775\n",
      "Epoch 34: saving model to checkpoints/20231129-175920/model_checkpoint_34.h5\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.0675 - accuracy: 0.1733\n",
      "Epoch 35/100\n",
      "3131/3131 [==============================] - 62s 20ms/step - loss: 4.0617 - accuracy: 0.1740\n",
      "Epoch 36/100\n",
      " 412/3131 [==>...........................] - ETA: 53s - loss: 3.9360 - accuracy: 0.1889\n",
      "Epoch 36: saving model to checkpoints/20231129-175920/model_checkpoint_36.h5\n",
      "3131/3131 [==============================] - 64s 20ms/step - loss: 4.0580 - accuracy: 0.1739\n",
      "Epoch 37/100\n",
      "2282/3131 [====================>.........] - ETA: 18s - loss: 4.0231 - accuracy: 0.1748\n",
      "Epoch 37: saving model to checkpoints/20231129-175920/model_checkpoint_37.h5\n",
      "3131/3131 [==============================] - 67s 21ms/step - loss: 4.0536 - accuracy: 0.1731\n",
      "Epoch 38/100\n",
      "3131/3131 [==============================] - 66s 21ms/step - loss: 4.0483 - accuracy: 0.1737\n",
      "Epoch 39/100\n",
      "1020/3131 [========>.....................] - ETA: 44s - loss: 3.9422 - accuracy: 0.1824\n",
      "Epoch 39: saving model to checkpoints/20231129-175920/model_checkpoint_39.h5\n",
      "3131/3131 [==============================] - 66s 21ms/step - loss: 4.0441 - accuracy: 0.1741\n",
      "Epoch 40/100\n",
      "2890/3131 [==========================>...] - ETA: 5s - loss: 4.0268 - accuracy: 0.1751\n",
      "Epoch 40: saving model to checkpoints/20231129-175920/model_checkpoint_40.h5\n",
      "3131/3131 [==============================] - 66s 21ms/step - loss: 4.0413 - accuracy: 0.1739\n",
      "Epoch 41/100\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.0365 - accuracy: 0.1746\n",
      "Epoch 42/100\n",
      "1628/3131 [==============>...............] - ETA: 31s - loss: 3.9649 - accuracy: 0.1790\n",
      "Epoch 42: saving model to checkpoints/20231129-175920/model_checkpoint_42.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.0336 - accuracy: 0.1738\n",
      "Epoch 43/100\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.0308 - accuracy: 0.1746\n",
      "Epoch 44/100\n",
      " 364/3131 [==>...........................] - ETA: 57s - loss: 3.8868 - accuracy: 0.1888\n",
      "Epoch 44: saving model to checkpoints/20231129-175920/model_checkpoint_44.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.0264 - accuracy: 0.1737\n",
      "Epoch 45/100\n",
      "2235/3131 [====================>.........] - ETA: 18s - loss: 3.9803 - accuracy: 0.1776\n",
      "Epoch 45: saving model to checkpoints/20231129-175920/model_checkpoint_45.h5\n",
      "3131/3131 [==============================] - 64s 21ms/step - loss: 4.0233 - accuracy: 0.1739\n",
      "Epoch 46/100\n",
      "3131/3131 [==============================] - 64s 21ms/step - loss: 4.0198 - accuracy: 0.1758\n",
      "Epoch 47/100\n",
      " 971/3131 [========>.....................] - ETA: 44s - loss: 3.9140 - accuracy: 0.1812\n",
      "Epoch 47: saving model to checkpoints/20231129-175920/model_checkpoint_47.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.0174 - accuracy: 0.1753\n",
      "Epoch 48/100\n",
      "2840/3131 [==========================>...] - ETA: 6s - loss: 4.0009 - accuracy: 0.1767\n",
      "Epoch 48: saving model to checkpoints/20231129-175920/model_checkpoint_48.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.0144 - accuracy: 0.1758\n",
      "Epoch 49/100\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.0108 - accuracy: 0.1754\n",
      "Epoch 50/100\n",
      "1578/3131 [==============>...............] - ETA: 32s - loss: 3.9501 - accuracy: 0.1780\n",
      "Epoch 50: saving model to checkpoints/20231129-175920/model_checkpoint_50.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.0089 - accuracy: 0.1752\n",
      "Epoch 51/100\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.0058 - accuracy: 0.1763\n",
      "Epoch 52/100\n",
      " 317/3131 [==>...........................] - ETA: 59s - loss: 3.8535 - accuracy: 0.1954\n",
      "Epoch 52: saving model to checkpoints/20231129-175920/model_checkpoint_52.h5\n",
      "3131/3131 [==============================] - 66s 21ms/step - loss: 4.0049 - accuracy: 0.1749\n",
      "Epoch 53/100\n",
      "2187/3131 [===================>..........] - ETA: 19s - loss: 3.9641 - accuracy: 0.1788\n",
      "Epoch 53: saving model to checkpoints/20231129-175920/model_checkpoint_53.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 4.0018 - accuracy: 0.1759\n",
      "Epoch 54/100\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 3.9989 - accuracy: 0.1768\n",
      "Epoch 55/100\n",
      " 923/3131 [=======>......................] - ETA: 45s - loss: 3.8894 - accuracy: 0.1845\n",
      "Epoch 55: saving model to checkpoints/20231129-175920/model_checkpoint_55.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 3.9969 - accuracy: 0.1750\n",
      "Epoch 56/100\n",
      "2793/3131 [=========================>....] - ETA: 6s - loss: 3.9833 - accuracy: 0.1768\n",
      "Epoch 56: saving model to checkpoints/20231129-175920/model_checkpoint_56.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 3.9948 - accuracy: 0.1760\n",
      "Epoch 57/100\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 3.9928 - accuracy: 0.1753\n",
      "Epoch 58/100\n",
      "1531/3131 [=============>................] - ETA: 33s - loss: 3.9195 - accuracy: 0.1808\n",
      "Epoch 58: saving model to checkpoints/20231129-175920/model_checkpoint_58.h5\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 3.9906 - accuracy: 0.1755\n",
      "Epoch 59/100\n",
      "3131/3131 [==============================] - 65s 21ms/step - loss: 3.9892 - accuracy: 0.1760\n",
      "Epoch 60/100\n",
      " 268/3131 [=>............................] - ETA: 59s - loss: 3.8583 - accuracy: 0.1925\n",
      "Epoch 60: saving model to checkpoints/20231129-175920/model_checkpoint_60.h5\n",
      "3131/3131 [==============================] - 66s 21ms/step - loss: 3.9867 - accuracy: 0.1761\n",
      "Epoch 61/100\n",
      "2137/3131 [===================>..........] - ETA: 19s - loss: 3.9442 - accuracy: 0.1803\n",
      "Epoch 61: saving model to checkpoints/20231129-175920/model_checkpoint_61.h5\n",
      "3131/3131 [==============================] - 60s 19ms/step - loss: 3.9854 - accuracy: 0.1763\n",
      "Epoch 62/100\n",
      "3131/3131 [==============================] - 58s 19ms/step - loss: 3.9829 - accuracy: 0.1765\n",
      "Epoch 63/100\n",
      " 876/3131 [=======>......................] - ETA: 42s - loss: 3.8803 - accuracy: 0.1868\n",
      "Epoch 63: saving model to checkpoints/20231129-175920/model_checkpoint_63.h5\n",
      "3131/3131 [==============================] - 63s 20ms/step - loss: 3.9809 - accuracy: 0.1763\n",
      "Epoch 64/100\n",
      "2746/3131 [=========================>....] - ETA: 7s - loss: 3.9639 - accuracy: 0.1778\n",
      "Epoch 64: saving model to checkpoints/20231129-175920/model_checkpoint_64.h5\n",
      "3131/3131 [==============================] - 58s 18ms/step - loss: 3.9799 - accuracy: 0.1765\n",
      "Epoch 65/100\n",
      "3131/3131 [==============================] - 59s 19ms/step - loss: 3.9775 - accuracy: 0.1759\n",
      "Epoch 66/100\n",
      "1484/3131 [=============>................] - ETA: 30s - loss: 3.9113 - accuracy: 0.1817\n",
      "Epoch 66: saving model to checkpoints/20231129-175920/model_checkpoint_66.h5\n",
      "3131/3131 [==============================] - 58s 19ms/step - loss: 3.9766 - accuracy: 0.1771\n",
      "Epoch 67/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9748 - accuracy: 0.1764\n",
      "Epoch 68/100\n",
      " 222/3131 [=>............................] - ETA: 53s - loss: 3.7974 - accuracy: 0.1947\n",
      "Epoch 68: saving model to checkpoints/20231129-175920/model_checkpoint_68.h5\n",
      "3131/3131 [==============================] - 58s 18ms/step - loss: 3.9735 - accuracy: 0.1772\n",
      "Epoch 69/100\n",
      "2090/3131 [===================>..........] - ETA: 19s - loss: 3.9186 - accuracy: 0.1821\n",
      "Epoch 69: saving model to checkpoints/20231129-175920/model_checkpoint_69.h5\n",
      "3131/3131 [==============================] - 58s 18ms/step - loss: 3.9717 - accuracy: 0.1769\n",
      "Epoch 70/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9703 - accuracy: 0.1765\n",
      "Epoch 71/100\n",
      " 827/3131 [======>.......................] - ETA: 42s - loss: 3.8907 - accuracy: 0.1848\n",
      "Epoch 71: saving model to checkpoints/20231129-175920/model_checkpoint_71.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9686 - accuracy: 0.1774\n",
      "Epoch 72/100\n",
      "2697/3131 [========================>.....] - ETA: 7s - loss: 3.9496 - accuracy: 0.1790\n",
      "Epoch 72: saving model to checkpoints/20231129-175920/model_checkpoint_72.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9675 - accuracy: 0.1776\n",
      "Epoch 73/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9658 - accuracy: 0.1774\n",
      "Epoch 74/100\n",
      "1434/3131 [============>.................] - ETA: 31s - loss: 3.9074 - accuracy: 0.1803\n",
      "Epoch 74: saving model to checkpoints/20231129-175920/model_checkpoint_74.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9641 - accuracy: 0.1771\n",
      "Epoch 75/100\n",
      "3131/3131 [==============================] - 58s 19ms/step - loss: 3.9628 - accuracy: 0.1774\n",
      "Epoch 76/100\n",
      " 172/3131 [>.............................] - ETA: 53s - loss: 3.8317 - accuracy: 0.1926\n",
      "Epoch 76: saving model to checkpoints/20231129-175920/model_checkpoint_76.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9623 - accuracy: 0.1767\n",
      "Epoch 77/100\n",
      "2041/3131 [==================>...........] - ETA: 19s - loss: 3.9191 - accuracy: 0.1813\n",
      "Epoch 77: saving model to checkpoints/20231129-175920/model_checkpoint_77.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9610 - accuracy: 0.1774\n",
      "Epoch 78/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9591 - accuracy: 0.1770\n",
      "Epoch 79/100\n",
      " 781/3131 [======>.......................] - ETA: 42s - loss: 3.8693 - accuracy: 0.1859\n",
      "Epoch 79: saving model to checkpoints/20231129-175920/model_checkpoint_79.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9582 - accuracy: 0.1772\n",
      "Epoch 80/100\n",
      "2649/3131 [========================>.....] - ETA: 8s - loss: 3.9363 - accuracy: 0.1794\n",
      "Epoch 80: saving model to checkpoints/20231129-175920/model_checkpoint_80.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9568 - accuracy: 0.1782\n",
      "Epoch 81/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9560 - accuracy: 0.1783\n",
      "Epoch 82/100\n",
      "1388/3131 [============>.................] - ETA: 31s - loss: 3.8976 - accuracy: 0.1841\n",
      "Epoch 82: saving model to checkpoints/20231129-175920/model_checkpoint_82.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9545 - accuracy: 0.1777\n",
      "Epoch 83/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9536 - accuracy: 0.1773\n",
      "Epoch 84/100\n",
      " 124/3131 [>.............................] - ETA: 55s - loss: 3.8611 - accuracy: 0.1951\n",
      "Epoch 84: saving model to checkpoints/20231129-175920/model_checkpoint_84.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9523 - accuracy: 0.1779\n",
      "Epoch 85/100\n",
      "1993/3131 [==================>...........] - ETA: 20s - loss: 3.9138 - accuracy: 0.1817\n",
      "Epoch 85: saving model to checkpoints/20231129-175920/model_checkpoint_85.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9515 - accuracy: 0.1782\n",
      "Epoch 86/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9507 - accuracy: 0.1786\n",
      "Epoch 87/100\n",
      " 732/3131 [======>.......................] - ETA: 43s - loss: 3.8516 - accuracy: 0.1892\n",
      "Epoch 87: saving model to checkpoints/20231129-175920/model_checkpoint_87.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9494 - accuracy: 0.1785\n",
      "Epoch 88/100\n",
      "2601/3131 [=======================>......] - ETA: 9s - loss: 3.9311 - accuracy: 0.1794\n",
      "Epoch 88: saving model to checkpoints/20231129-175920/model_checkpoint_88.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9490 - accuracy: 0.1779\n",
      "Epoch 89/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9469 - accuracy: 0.1773\n",
      "Epoch 90/100\n",
      "1339/3131 [===========>..................] - ETA: 32s - loss: 3.8722 - accuracy: 0.1882\n",
      "Epoch 90: saving model to checkpoints/20231129-175920/model_checkpoint_90.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9465 - accuracy: 0.1791\n",
      "Epoch 91/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9454 - accuracy: 0.1781\n",
      "Epoch 92/100\n",
      "  76/3131 [..............................] - ETA: 55s - loss: 3.8118 - accuracy: 0.1982\n",
      "Epoch 92: saving model to checkpoints/20231129-175920/model_checkpoint_92.h5\n",
      "3131/3131 [==============================] - 58s 19ms/step - loss: 3.9456 - accuracy: 0.1782\n",
      "Epoch 93/100\n",
      "1946/3131 [=================>............] - ETA: 21s - loss: 3.9010 - accuracy: 0.1821\n",
      "Epoch 93: saving model to checkpoints/20231129-175920/model_checkpoint_93.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9440 - accuracy: 0.1782\n",
      "Epoch 94/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9429 - accuracy: 0.1780\n",
      "Epoch 95/100\n",
      " 684/3131 [=====>........................] - ETA: 44s - loss: 3.8502 - accuracy: 0.1892\n",
      "Epoch 95: saving model to checkpoints/20231129-175920/model_checkpoint_95.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9420 - accuracy: 0.1783\n",
      "Epoch 96/100\n",
      "2553/3131 [=======================>......] - ETA: 10s - loss: 3.9214 - accuracy: 0.1819\n",
      "Epoch 96: saving model to checkpoints/20231129-175920/model_checkpoint_96.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9409 - accuracy: 0.1794\n",
      "Epoch 97/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9403 - accuracy: 0.1783\n",
      "Epoch 98/100\n",
      "1291/3131 [===========>..................] - ETA: 33s - loss: 3.8644 - accuracy: 0.1859\n",
      "Epoch 98: saving model to checkpoints/20231129-175920/model_checkpoint_98.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9394 - accuracy: 0.1788\n",
      "Epoch 99/100\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9392 - accuracy: 0.1784\n",
      "Epoch 100/100\n",
      "  28/3131 [..............................] - ETA: 55s - loss: 3.9084 - accuracy: 0.1975\n",
      "Epoch 100: saving model to checkpoints/20231129-175920/model_checkpoint_100.h5\n",
      "3131/3131 [==============================] - 57s 18ms/step - loss: 3.9387 - accuracy: 0.1790\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x2f9e6c090>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "checkpoint_path = \"checkpoints/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\" + \"model_checkpoint_{epoch:02d}.h5\"\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(checkpoint_path, save_freq=5000, verbose=1)\n",
    "\n",
    "model.fit(X, y,\n",
    "          epochs=100, verbose=1,\n",
    "          callbacks=[tensorboard_callback, checkpoint_callback])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63a05607452391c3",
    "outputId": "ba06257a-8d6e-4401-ecdf-42babe358acb",
    "ExecuteTime": {
     "end_time": "2023-11-29T13:56:27.964889Z",
     "start_time": "2023-11-29T12:14:20.693879Z"
    }
   },
   "id": "63a05607452391c3"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels/model_\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m.h5\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(datetime\u001B[38;5;241m.\u001B[39mnow())\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m), save_format\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh5\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels/tokenizer_\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(datetime\u001B[38;5;241m.\u001B[39mnow())\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m tokenizer_file:\n\u001B[1;32m      6\u001B[0m     pickle\u001B[38;5;241m.\u001B[39mdump(tokenizer, tokenizer_file, protocol\u001B[38;5;241m=\u001B[39mpickle\u001B[38;5;241m.\u001B[39mHIGHEST_PROTOCOL)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model.save(\"models/model_{0}.h5\".format(datetime.now()).replace(\" \", \"_\"), save_format='h5')\n",
    "\n",
    "with open(\"models/tokenizer_{0}.pickle\".format(datetime.now()).replace(\" \", \"_\"), 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c96235814e505f4b",
    "outputId": "a2298948-0f60-453c-aa28-5ee03647469c",
    "ExecuteTime": {
     "end_time": "2023-11-30T03:29:27.050758Z",
     "start_time": "2023-11-30T03:29:26.944258Z"
    }
   },
   "id": "c96235814e505f4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed_text = \"Hello there, do you i am\"\n",
    "next_words = 1\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences(\n",
    "        [token_list],\n",
    "        maxlen=2,\n",
    "        padding='pre'\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(token_list)\n",
    "    pred_word = tokenizer.index_word[np.argmax(predictions)]\n",
    "    seed_text += \" \" + pred_word\n",
    "\n",
    "print(\"Next predicted words: \", seed_text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f03606a6d69bdd6c",
    "outputId": "232b2b31-9518-4928-9342-36703d255daa"
   },
   "id": "f03606a6d69bdd6c"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_l = [\"hello\", \"word\"]\n",
    "w_r = [\"hello\", \"word\"]\n",
    "\n",
    "\n",
    "def f():\n",
    "    for word in w_r:\n",
    "        \n",
    "        if word in w_l:\n",
    "            continue\n",
    "        else:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "f()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T05:00:51.518220Z",
     "start_time": "2023-11-30T05:00:51.499081Z"
    }
   },
   "id": "e3f770db89be1f94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "72fa38655f06e5c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "V100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
